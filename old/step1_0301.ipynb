{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67745544-a114-42f6-a729-11570329f3a1",
   "metadata": {},
   "source": [
    "# Step1 : \n",
    "## 1. 100 train-test split on dataset for experiment.\n",
    "## 2. 100 super set generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ccbc907-064a-4c62-85d3-d434362d1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_ipcw, concordance_index_censored\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# models \n",
    "from lifelines import CoxPHFitter\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# others\n",
    "from numpy import inf\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c165b40-4421-4847-836c-41665fbdf292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENS SURV module\n",
    "from ens_surv.utils import *\n",
    "from ens_surv.boot_kfold import boot_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4aab2ec-2fc5-4b3f-b384-16c07c31dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "# loading data & preprop\n",
    "\n",
    "# settings \n",
    "dir = \"/Users/pio/Google 드라이브/data/\"\n",
    "file_name = \"pbc2.csv\"\n",
    "data = pd.read_csv(dir + file_name)\n",
    "\n",
    "# drop status1 - competing risks setting\n",
    "data = data.drop(axis=1, columns =['status'])\n",
    "\n",
    "\n",
    "# ID, Time, Event, Measure Time column names\n",
    "ID_col = 'id'; T_col ='years'; E_col ='status2'; measure_T_col = 'year'\n",
    "\n",
    "# categorical variables\n",
    "nominal_col = ['drug','sex', 'ascites', 'hepatomegaly','spiders', 'edema']\n",
    "ordinal_col = ['histologic']\n",
    "\n",
    "# continuous variables\n",
    "cont_col = list(set(data.columns) - set(nominal_col) - set(ordinal_col) - set([ID_col, T_col, E_col, measure_T_col]))\n",
    "\n",
    "# window - 5 year prediction \n",
    "window = 5\n",
    "\n",
    "# S : landmark time points - 0, 0.5, 1, ..., 10\n",
    "S = np.linspace(0,10,21)\n",
    "v_years = S+window\n",
    "\n",
    "# Number of bins when discritizing \n",
    "## !!!(Actually, k_bin - 1 bins are produced)!!!\n",
    "k_bin = 5\n",
    "\n",
    "# minimal bin_size\n",
    "minimal_bin_size = window / (k_bin-1)\n",
    "# t_grid -> minimal points where survival probabilities are measured\n",
    "# t_grid = np.arange(0,S[-1] + window + minimal_bin_size, step = minimal_bin_size)\n",
    "\n",
    "# imputation -> fill na's : median for continous\n",
    "for col in cont_col : \n",
    "    data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "\n",
    "# one-hot encoding for categorical variables\n",
    "data = pd.get_dummies(data, columns = nominal_col, drop_first=True)\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "# settings2\n",
    "\n",
    "# proportion of train set\n",
    "p_train = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8cf26-71c1-4171-8706-20e7c662f4b4",
   "metadata": {},
   "source": [
    "# 1. 100 train-test split & Stacking on dataset for experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ebc89d-5675-45cc-a208-9d2d48db8377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nscaler = MinMaxScaler()\\n\\nfor i in range(100) : \\n    random.seed(i)\\n    train, test = splitID(data = data,ID_col = ID_col, p = p_train)\\n    \\n#    print(train.shape)\\n#    print(test.shape)\\n    print('seed : '+ str(i))\\n    print('Intersection : ', set(np.unique(train[ID_col])).intersection(set(np.unique(test[ID_col]))))\\n\\n    feature_cols = ['age','serBilir', 'serChol', 'albumin','alkaline', 'SGOT', 'platelets', 'prothrombin', 'histologic', 'status2','drug_placebo', 'sex_male', 'ascites_Yes', 'hepatomegaly_Yes',\\n'spiders_Yes', 'edema_edema despite diuretics','edema_edema no diuretics']\\n\\n\\n    train[feature_cols] = scaler.fit_transform(train[feature_cols])\\n    test[feature_cols] = scaler.transform(test[feature_cols])\\n\\n\\n    train_lm1 = LM_transformer(df=train,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col)\\n    test_lm1 = LM_transformer(df=test,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col)\\n\\n    train_lm2_train_ver = LM_transformer2(df=train_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=True)\\n    train_lm2_validation_ver = LM_transformer2(df=train_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=False)\\n\\n    test_lm2 = LM_transformer2(df=test_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=False)\\n    \\n    # write file\\n    train.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train'+'.csv',index=False)\\n    test.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_test'+'.csv',index=False)\\n    \\n    train_lm1.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train_lm1'+'.csv',index=False)\\n    test_lm1.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_test_lm1'+'.csv',index=False)\\n    \\n    train_lm2_train_ver.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train_lm2_train_ver'+'.csv',index=False)\\n    train_lm2_validation_ver.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train_lm2_validation_ver'+'.csv',index=False)\\n    test_lm2.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_test_lm2'+'.csv',index=False)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "feature_cols = ['age','serBilir', 'serChol', 'albumin','alkaline', 'SGOT', 'platelets', 'prothrombin', 'histologic', 'status2','drug_placebo', 'sex_male', 'ascites_Yes', 'hepatomegaly_Yes',\n",
    "'spiders_Yes', 'edema_edema despite diuretics','edema_edema no diuretics']\n",
    "\n",
    "\n",
    "for i in range(100) : \n",
    "    random.seed(i)\n",
    "    train, test = splitID(data = data,ID_col = ID_col, p = p_train)\n",
    "    \n",
    "#    print(train.shape)\n",
    "#    print(test.shape)\n",
    "#    print('seed : '+ str(i))\n",
    "#    print('Intersection : ', set(np.unique(train[ID_col])).intersection(set(np.unique(test[ID_col]))))\n",
    "\n",
    "    train[feature_cols] = scaler.fit_transform(train[feature_cols])\n",
    "    test[feature_cols] = scaler.transform(test[feature_cols])\n",
    "\n",
    "\n",
    "    train_lm1 = LM_transformer(df=train,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col)\n",
    "    test_lm1 = LM_transformer(df=test,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col)\n",
    "\n",
    "    train_lm2_train_ver = LM_transformer2(df=train_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=True)\n",
    "    train_lm2_validation_ver = LM_transformer2(df=train_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=False)\n",
    "\n",
    "    test_lm2 = LM_transformer2(df=test_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=False)\n",
    "    \n",
    "    # write file\n",
    "    train.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train'+'.csv',index=False)\n",
    "    test.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_test'+'.csv',index=False)\n",
    "    \n",
    "    train_lm1.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train_lm1'+'.csv',index=False)\n",
    "    test_lm1.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_test_lm1'+'.csv',index=False)\n",
    "    \n",
    "    train_lm2_train_ver.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train_lm2_train_ver'+'.csv',index=False)\n",
    "    train_lm2_validation_ver.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train_lm2_validation_ver'+'.csv',index=False)\n",
    "    test_lm2.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_test_lm2'+'.csv',index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312aeb15-cb83-4b52-802d-c436ba84d96f",
   "metadata": {},
   "source": [
    "# 2. 100 super set generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621fd4c1-e74c-4be1-ba42-3829d45a7507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# setting : \\n\\n# B : number of resampling / K : number of folds / boot : replacement true false\\nB = 1; K = 3; boot = False\\n\\nbase_info = {'ID_col':ID_col, 'T_col':T_col, 'E_col':E_col, 'measure_T_col':measure_T_col, 'boot':boot, 'B':B, 'K':K, \\n            'window':window , 'S' :S, 'k_bin':k_bin}\\n\\n# model specifics : model name & model instance & hyperparameter grid & type of model\\n## type of model : cont(continous) or disc(discrete)\\n\\n## model specifics of level 1 models\\ncox1_params = {'penalizer':[0,0.05,0.1,0.5],'l1_ratio':[0,0.25,0.5,0.75,1]}\\n\\nmodel_specifics_cont = pd.DataFrame({'model_name' : ['cox1'], \\n                                'model_instance':[CoxPHFitter()], \\n                                'hyperparams':[cox1_params], \\n                                'type':['cont']})\\n\\nLR_params = {'C':[0.05,  10]}\\nRF_params = {'n_estimators':[50,100,300,500],'max_depth':[1,3,5]}\\nGB_params = {'n_estimators':[50,100,300,500],'max_depth':[1,3,5]}\\nMLP_params = {'hidden_layer_sizes':[1,2,3], 'activation' : ['identity', 'logistic', 'tanh', 'relu'], 'max_iter' : [1000], 'early_stopping' : [True], 'learning_rate' : ['adaptive']}\\nKNN_params = {'n_neighbors':[1,5,10], 'weights':['uniform', 'distance']}\\nNGB_params = {'var_smoothing':[1e-5, 1e-9, 1e-1]}\\nADA_params = {'n_estimators':[50,100,300,500], 'learning_rate':[0.1,0.25,0.75,1]}\\n\\n\\n\\nmodel_specifics_disc = pd.DataFrame({'model_name' : ['LR','RF','GB','MLP','KNN','NGB','ADA'], \\n                                'model_instance':[LogisticRegression(max_iter=10000),RandomForestClassifier(),GradientBoostingClassifier(),MLPClassifier(),KNeighborsClassifier(),GaussianNB(), AdaBoostClassifier()], \\n                                'hyperparams':[LR_params, RF_params, GB_params,MLP_params, KNN_params,NGB_params, ADA_params], \\n                                'type':['disc','disc','disc','disc','disc','disc','disc']})\\n\\n\\nmodel_specifics_1 = pd.concat([model_specifics_cont,model_specifics_disc],axis=0).reset_index(drop=True)\\n\\n## model specifics of level 2 models\\nmodel_specifics_2 = pd.DataFrame({'model_name':['M1'], \\n                                  'model_instance':[LogisticRegression(max_iter=10000)],\\n                                  'hyperparams':[{'C':[0.05, 10]}],\\n                                 })\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# setting : \n",
    "\n",
    "# B : number of resampling / K : number of folds / boot : replacement true false\n",
    "B = 1; K = 3; boot = False\n",
    "\n",
    "base_info = {'ID_col':ID_col, 'T_col':T_col, 'E_col':E_col, 'measure_T_col':measure_T_col, 'boot':boot, 'B':B, 'K':K, \n",
    "            'window':window , 'S' :S, 'k_bin':k_bin}\n",
    "\n",
    "# model specifics : model name & model instance & hyperparameter grid & type of model\n",
    "## type of model : cont(continous) or disc(discrete)\n",
    "\n",
    "## model specifics of level 1 models\n",
    "cox1_params = {'penalizer':[0,0.05,0.1,0.5],'l1_ratio':[0,0.25,0.5,0.75,1]}\n",
    "\n",
    "model_specifics_cont = pd.DataFrame({'model_name' : ['cox1'], \n",
    "                                'model_instance':[CoxPHFitter()], \n",
    "                                'hyperparams':[cox1_params], \n",
    "                                'type':['cont']})\n",
    "\n",
    "LR_params = {'C':[0.05,  10]}\n",
    "RF_params = {'n_estimators':[50,100,300,500],'max_depth':[1,3,5]}\n",
    "GB_params = {'n_estimators':[50,100,300,500],'max_depth':[1,3,5]}\n",
    "MLP_params = {'hidden_layer_sizes':[1,2,3], 'activation' : ['identity', 'logistic', 'tanh', 'relu'], 'max_iter' : [1000], 'early_stopping' : [True], 'learning_rate' : ['adaptive']}\n",
    "KNN_params = {'n_neighbors':[1,5,10], 'weights':['uniform', 'distance']}\n",
    "NGB_params = {'var_smoothing':[1e-5, 1e-9, 1e-1]}\n",
    "ADA_params = {'n_estimators':[50,100,300,500], 'learning_rate':[0.1,0.25,0.75,1]}\n",
    "\n",
    "\n",
    "\n",
    "model_specifics_disc = pd.DataFrame({'model_name' : ['LR','RF','GB','MLP','KNN','NGB','ADA'], \n",
    "                                'model_instance':[LogisticRegression(max_iter=10000),RandomForestClassifier(),GradientBoostingClassifier(),MLPClassifier(),KNeighborsClassifier(),GaussianNB(), AdaBoostClassifier()], \n",
    "                                'hyperparams':[LR_params, RF_params, GB_params,MLP_params, KNN_params,NGB_params, ADA_params], \n",
    "                                'type':['disc','disc','disc','disc','disc','disc','disc']})\n",
    "\n",
    "\n",
    "model_specifics_1 = pd.concat([model_specifics_cont,model_specifics_disc],axis=0).reset_index(drop=True)\n",
    "\n",
    "## model specifics of level 2 models\n",
    "model_specifics_2 = pd.DataFrame({'model_name':['M1'], \n",
    "                                  'model_instance':[LogisticRegression(max_iter=10000)],\n",
    "                                  'hyperparams':[{'C':[0.05, 10]}],\n",
    "                                 })\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd4cadc-13aa-484a-9e88-50beb6a82def",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# model 정보 - 순서대로...\\nfor g_1 in range(model_specifics_1.shape[0]) :\\n    model_name = model_specifics_1.loc[g_1,'model_name'] \\n    model_instance = model_specifics_1.loc[g_1,'model_instance']\\n    model_hyperparams = model_specifics_1.loc[g_1,'hyperparams']\\n    model_type = model_specifics_1.loc[g_1,'type']\\n    \\n    param_combinations = list(itertools.product(*list(model_hyperparams.values())))\\n    param_names = list(model_hyperparams.keys())\\n    \\n    print(model_name)\\n    print(model_instance)\\n    print(param_combinations)\\n    print(param_names)    \\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# model 정보 - 순서대로...\n",
    "for g_1 in range(model_specifics_1.shape[0]) :\n",
    "    model_name = model_specifics_1.loc[g_1,'model_name'] \n",
    "    model_instance = model_specifics_1.loc[g_1,'model_instance']\n",
    "    model_hyperparams = model_specifics_1.loc[g_1,'hyperparams']\n",
    "    model_type = model_specifics_1.loc[g_1,'type']\n",
    "    \n",
    "    param_combinations = list(itertools.product(*list(model_hyperparams.values())))\n",
    "    param_names = list(model_hyperparams.keys())\n",
    "    \n",
    "    print(model_name)\n",
    "    print(model_instance)\n",
    "    print(param_combinations)\n",
    "    print(param_names)    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea59585-af74-4178-9359-31d6259bacb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndir_temp = '/Users/pio/Google 드라이브/github/survival ensemble/dataset/pbc2'\\n\\n# Read ith dataset \\nfor i in range(100) : \\n    # directory of ith sets\\n    train_dir = dir_temp+'_seed_'+str(i)+'_'+'train.csv'\\n    test_dir = dir_temp+'_seed_'+str(i)+'_'+'test.csv'\\n\\n    train_lm1_dir = dir_temp+'_seed_'+str(i)+'_'+'train_lm1.csv'\\n    test_lm1_dir = dir_temp+'_seed_'+str(i)+'_'+'test_lm1.csv'\\n\\n    train_lm2_train_ver_dir = dir_temp+'_seed_'+str(i)+'_'+'train_lm2_train_ver.csv'\\n    train_lm2_validation_ver_dir = dir_temp+'_seed_'+str(i)+'_'+'train_lm2_validation_ver.csv'\\n    test_lm2_dir = dir_temp+'_seed_'+str(i)+'_'+'test_lm2.csv'\\n\\n    # read ith sets\\n    train = pd.read_csv(train_dir)\\n    test = pd.read_csv(test_dir)\\n\\n    train_lm1 = pd.read_csv(train_lm1_dir)\\n    test_lm1 = pd.read_csv(test_lm1_dir)\\n\\n    train_lm2_train_ver = pd.read_csv(train_lm2_train_ver_dir)\\n    train_lm2_validation_ver = pd.read_csv(train_lm2_validation_ver_dir)\\n    test_lm2 = pd.read_csv(test_lm2_dir)\\n\\n    # super set(stacking)\\n    \\n    train_df_list = [train, train_lm1, train_lm2_train_ver, train_lm2_validation_ver]\\n    test_df_list = [test, test_lm1, test_lm2]\\n\\n    \\n    stacked_noboot = boot_kfold(base_info = base_info, train_df_list = train_df_list, \\n           test_df_list = test_df_list,\\n           model_specifics_1 = model_specifics_1, \\n           model_specifics_2 = model_specifics_2)\\n    \\n    \\n    stacked_noboot.boot_stack()\\n\\n    \\n    # store supersets\\n    pd.DataFrame(stacked_noboot.train_supersets[0][:,1:]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'train_stack_X.csv',index=False)\\n    pd.DataFrame(stacked_noboot.train_supersets[0][:,0]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'train_stack_y.csv',index=False)\\n    pd.DataFrame(stacked_noboot.weights[0]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'train_stack_w.csv',index=False)\\n\\n    pd.DataFrame(stacked_noboot.test_superset[0][:,1:]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'test_stack_X.csv',index=False)\\n    pd.DataFrame(stacked_noboot.test_superset[0][:,0]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'test_stack_y.csv',index=False)\\n\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dir_temp = '/Users/pio/Google 드라이브/github/survival ensemble/dataset/pbc2'\n",
    "\n",
    "# Read ith dataset \n",
    "for i in range(100) : \n",
    "    # directory of ith sets\n",
    "    train_dir = dir_temp+'_seed_'+str(i)+'_'+'train.csv'\n",
    "    test_dir = dir_temp+'_seed_'+str(i)+'_'+'test.csv'\n",
    "\n",
    "    train_lm1_dir = dir_temp+'_seed_'+str(i)+'_'+'train_lm1.csv'\n",
    "    test_lm1_dir = dir_temp+'_seed_'+str(i)+'_'+'test_lm1.csv'\n",
    "\n",
    "    train_lm2_train_ver_dir = dir_temp+'_seed_'+str(i)+'_'+'train_lm2_train_ver.csv'\n",
    "    train_lm2_validation_ver_dir = dir_temp+'_seed_'+str(i)+'_'+'train_lm2_validation_ver.csv'\n",
    "    test_lm2_dir = dir_temp+'_seed_'+str(i)+'_'+'test_lm2.csv'\n",
    "\n",
    "    # read ith sets\n",
    "    train = pd.read_csv(train_dir)\n",
    "    test = pd.read_csv(test_dir)\n",
    "\n",
    "    train_lm1 = pd.read_csv(train_lm1_dir)\n",
    "    test_lm1 = pd.read_csv(test_lm1_dir)\n",
    "\n",
    "    train_lm2_train_ver = pd.read_csv(train_lm2_train_ver_dir)\n",
    "    train_lm2_validation_ver = pd.read_csv(train_lm2_validation_ver_dir)\n",
    "    test_lm2 = pd.read_csv(test_lm2_dir)\n",
    "\n",
    "    # super set(stacking)\n",
    "    \n",
    "    train_df_list = [train, train_lm1, train_lm2_train_ver, train_lm2_validation_ver]\n",
    "    test_df_list = [test, test_lm1, test_lm2]\n",
    "\n",
    "    \n",
    "    stacked_noboot = boot_kfold(base_info = base_info, train_df_list = train_df_list, \n",
    "           test_df_list = test_df_list,\n",
    "           model_specifics_1 = model_specifics_1, \n",
    "           model_specifics_2 = model_specifics_2)\n",
    "    \n",
    "    \n",
    "    stacked_noboot.boot_stack()\n",
    "\n",
    "    \n",
    "    # store supersets\n",
    "    pd.DataFrame(stacked_noboot.train_supersets[0][:,1:]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'train_stack_X.csv',index=False)\n",
    "    pd.DataFrame(stacked_noboot.train_supersets[0][:,0]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'train_stack_y.csv',index=False)\n",
    "    pd.DataFrame(stacked_noboot.weights[0]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'train_stack_w.csv',index=False)\n",
    "\n",
    "    pd.DataFrame(stacked_noboot.test_superset[0][:,1:]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'test_stack_X.csv',index=False)\n",
    "    pd.DataFrame(stacked_noboot.test_superset[0][:,0]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'test_stack_y.csv',index=False)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006bebdd-a88f-4351-8442-6ac4a5a83308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c24fae5b-b3e1-4830-8eb0-3d1e43763f4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d8cd7-8706-4a77-936d-3fd3a4d89b42",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b42948-b548-40d6-bda7-325ded3699e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
