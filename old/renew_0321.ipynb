{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382d444f-946b-44c3-a05c-33948e852874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_ipcw, concordance_index_censored\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# models \n",
    "from lifelines import CoxPHFitter\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# others\n",
    "from numpy import inf\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cec73f0-72b0-4e16-b434-eb9003ae9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENS SURV module\n",
    "# from ens_surv.utils import *\n",
    "# from ens_surv.boot_kfold import boot_kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314000cb-2252-4d71-a9b3-0e7ec929e0c0",
   "metadata": {},
   "source": [
    "# boot kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4688d9b2-758b-4e81-a0f7-e291d61988a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_ipcw, concordance_index_censored\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# models \n",
    "from lifelines import CoxPHFitter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# others\n",
    "from numpy import inf\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "import copy\n",
    "from sklearn.base import clone\n",
    "\n",
    "from ens_surv.utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Return bootstrapped Superset, train set(in-bag), oob sample datasets.\n",
    "# Return bootstrapped Superset, train set(in-bag), oob sample datasets.\n",
    "class boot_kfold :\n",
    "    def __init__(self, base_info, train_df_list, test_df_list,model_specifics_1, model_specifics_2) :         \n",
    "        # base_info : dict with ID_col, T_col, E_col, measure_T_col names, boot(bool), B, K\n",
    "        self.base_info = base_info\n",
    "        self.ID_col = base_info['ID_col']\n",
    "        self.T_col = base_info['T_col']\n",
    "        self.E_col = base_info['E_col']\n",
    "        self.measure_T_col = base_info['measure_T_col']\n",
    "        self.window = base_info['window']\n",
    "        self.S = base_info['S']\n",
    "        self.k_bin = base_info['k_bin']\n",
    "        \n",
    "        self.boot = base_info['boot']\n",
    "        self.B = base_info['B']\n",
    "        self.K = base_info['K']\n",
    "        \n",
    "        # sorting dataframes in right order\n",
    "        temp = [train_df_list[0]]\n",
    "        for df in train_df_list[1:] :\n",
    "            temp.append(df.sort_values(['LM',self.ID_col]))\n",
    "        train_df_list = temp\n",
    "        \n",
    "        temp = [test_df_list[0]]\n",
    "        for df in test_df_list[1:] :\n",
    "            temp.append(df.sort_values(['LM',self.ID_col]))\n",
    "        test_df_list = temp\n",
    "        \n",
    "        del(temp)\n",
    "\n",
    "        # list of dataframes :\n",
    "        ## in train, sequently, original data / lm1 transformed / lm2 transformed(trn form) / lm2 transformed(validation form)\n",
    "        ## in test, sequently, original data / lm1 transformed/ lm2 transformed(validation form)\n",
    "        self.train_df_list = train_df_list\n",
    "        self.test_df_list = test_df_list\n",
    "        \n",
    "        # model_specifics(dataframe)\n",
    "        ## model_specifics_1 : 1st stage models' 1) model name / model_instance / hyperparams grid / type\n",
    "        ## model_specifics_2 : 2nd stage models' 1) model name / model_instance / hyperparams grid / type\n",
    "        self.model_specifics_1 = model_specifics_1\n",
    "        self.model_specifics_2 = model_specifics_2\n",
    "    \n",
    "    # boot_stack outputs B stacked super set\n",
    "    def boot_stack(self,train_df_list = None, test_df_list = None, model_specifics_1 = None, model_specifics_2 = None, \n",
    "                   ID_col = None,T_col=None,E_col=None,measure_T_col= None,\n",
    "                   window = None, S = None, k_bin = None,\n",
    "                   boot = None, B = None, K= None) : \n",
    "        # initiallizing\n",
    "        if train_df_list is None :\n",
    "            train_df_list = self.train_df_list\n",
    "        if test_df_list is None :\n",
    "            test_df_list = self.test_df_list\n",
    "            \n",
    "        if model_specifics_1 is None :\n",
    "            model_specifics_1 = self.model_specifics_1\n",
    "        if model_specifics_2 is None :\n",
    "            model_specifics_2 = self.model_specifics_2\n",
    "\n",
    "        if ID_col is None :\n",
    "            ID_col = self.ID_col\n",
    "        if E_col is None :\n",
    "            E_col = self.E_col\n",
    "        if T_col is None :\n",
    "            T_col = self.T_col\n",
    "        if measure_T_col is None :\n",
    "            measure_T_col = self.measure_T_col\n",
    "        if window is None :\n",
    "            window = self.window\n",
    "        if S is None :\n",
    "            S = self.S\n",
    "        if k_bin is None :\n",
    "            k_bin = self.k_bin\n",
    "        \n",
    "        if boot is None :\n",
    "            boot = self.boot\n",
    "        if B is None :\n",
    "            B = self.B\n",
    "        if K is None :\n",
    "            K = self.K\n",
    "        \n",
    "        # censoring model\n",
    "        KM_cens = KaplanMeierFitter()\n",
    "                    \n",
    " ######################################################################################################################################################################\n",
    "        # OUTER-LOOP\n",
    "        BOOTSTRAP_SUPERSETS = []; IN_BAG_SETS = []; OUT_BAG_SETS = []; WEIGHT_BAG_SETS = []; TEST_SUPER_SET = []\n",
    "        \n",
    "        for b in range(B) :\n",
    "            print('######################################################################')\n",
    "            print(b+1,'/', B,' Resampled')\n",
    "            \n",
    "            # add bootstrap weight and IPC weight column to the inbag sets.\n",
    "            train_df_list_new, train_df_list_oob = add_weight_column(train_df_list=train_df_list, ID_col = ID_col, T_col=T_col, E_col = E_col, boot=boot, S=S, window= window)\n",
    "            \n",
    "            # kfold part - Different IDs are divided into K folds\n",
    "            kf = kfold(k=K, ID_col=ID_col, df1 = train_df_list_new[0], df2 = train_df_list_new[1], df3_train = train_df_list_new[2], df3_validation = train_df_list_new[3])\n",
    "            ############################################################################################################\n",
    "            # INNER-LOOP(k-fold)\n",
    "            ## b_TH_STACK : 1st column contains true survival status / 2 to end columns contain survival estimates(of training set) from different models. \n",
    "            b_TH_STACK = np.array([])\n",
    "            b_TH_weight = []\n",
    "            for k in range(K) :\n",
    "                print(k+1,'/', K,' fold')\n",
    "                # \n",
    "                df1_k_train, df1_k_validation, df2_k_train, df2_k_validation, df3_k_train, df3_k_validation = next(kf)\n",
    "\n",
    "                # Training 1st stage models\n",
    "                ## 1) Training 1st stage models with kth training set\n",
    "                ## 2) Predict kth validation set with trained 1st stage models\n",
    "                ## Stacking results from 2), forming inputs for 2nd stage models\n",
    "\n",
    "                out_b_k = level_1_stack(model_specifics_1,ID_col=ID_col, E_col=E_col, T_col = T_col, measure_T_col = measure_T_col, window = window, S = S, k_bin = k_bin, \n",
    "                                        train_sets=[df1_k_train, df2_k_train, df3_k_train], \n",
    "                                        validation_sets=[df1_k_validation, df2_k_validation, df3_k_validation])\n",
    "                weight_b_k = df2_k_validation['weight']\n",
    "                \n",
    "                b_TH_STACK = b_TH_STACK.reshape(-1, out_b_k.shape[1])\n",
    "                b_TH_STACK = np.vstack((b_TH_STACK, out_b_k))\n",
    "                \n",
    "                b_TH_weight = np.append(b_TH_weight, np.array(weight_b_k).ravel())\n",
    "            ############################################################################################################            \n",
    "            # append results from bth bootstrapping, b = 1, ... , B\n",
    "            ## BOOTSTRAP_SUPERSETS : All B (b_TH_STACK) super sets obtained from B bootstrap samples.\n",
    "            BOOTSTRAP_SUPERSETS.append(b_TH_STACK)\n",
    "            ## in_bag_train : to fully train 1st stage models\n",
    "            IN_BAG_SETS.append(train_df_list_new)\n",
    "        \n",
    "            ## out_bag_train : to check validity\n",
    "            OUT_BAG_SETS.append(train_df_list_oob)\n",
    "            \n",
    "            # Weights to use for 2nd stage model\n",
    "            WEIGHT_BAG_SETS.append(b_TH_weight)\n",
    "            \n",
    "            \n",
    "            # Refit 1st stage model -> Store test set & oob set predictions\n",
    "            ## oob sets... later\n",
    "            b_TH_test_superset = level_1_stack(model_specifics_1,ID_col=ID_col, E_col=E_col, T_col = T_col, measure_T_col = measure_T_col, \n",
    "                                   window = window, S = S, k_bin = k_bin, \n",
    "                                   train_sets=[IN_BAG_SETS[b][0],IN_BAG_SETS[b][1],IN_BAG_SETS[b][2]], validation_sets=test_df_list)\n",
    "            \n",
    "            TEST_SUPER_SET.append(b_TH_test_superset)\n",
    "            # Fit 2nd stage model & Store( bootstrap True / False 에 따라 다르겠지?)\n",
    "            ## To be continue... 이건 다른 class로 빼자! 여기서는 stacking만...\n",
    "            \n",
    " ###################################################################################################################################################################### \n",
    "        \n",
    "        # store \n",
    "        self.inbags = IN_BAG_SETS\n",
    "        self.outbags = OUT_BAG_SETS\n",
    "        \n",
    "        self.train_supersets = BOOTSTRAP_SUPERSETS\n",
    "        self.test_superset = TEST_SUPER_SET\n",
    "        \n",
    "        self.weights = WEIGHT_BAG_SETS \n",
    "\n",
    "        # df1_k_train, df1_k_validation, df2_k_train, df2_k_validation, df3_k_train, df3_k_validation\n",
    "        return BOOTSTRAP_SUPERSETS, IN_BAG_SETS, OUT_BAG_SETS, WEIGHT_BAG_SETS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31529b18-c6cb-4302-bb57-a2e65e1c6cb2",
   "metadata": {},
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2617f4b-0db9-4778-bdab-054108d9b022",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_ipcw, concordance_index_censored\n",
    "\n",
    "# models \n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "# others\n",
    "from numpy import inf\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "import copy\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_squared_error,brier_score_loss\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "# squared error \n",
    "def sq_error(a,b) : \n",
    "    return( np.mean( (a-b)**2 ))\n",
    "\n",
    "## Data Gen\n",
    "def LM_transformer(df,ID_col, T_col,E_col,window,S,measure_T_col) :\n",
    "    super_set = pd.DataFrame()\n",
    "    \n",
    "    for t in S :\n",
    "        # LM point 이후 생존자\n",
    "        # R_t_idx = np.where(df[T_col] > t )\n",
    "        R_t_idx = np.where( (df[T_col] > t ) & (df[measure_T_col] <= t ) )\n",
    "        R_t = df.loc[R_t_idx].reset_index(drop=True)\n",
    "        \n",
    "        # LM point - 변수로 지정. strata로 나중에 지정하려고\n",
    "        R_t['LM'] = t\n",
    "        \n",
    "        # time & event 수정 필요한 그룹. -> t+w 시점에서 censoring된 것으로 처리\n",
    "        occurance_out_index = np.where(R_t[T_col] > t+window)\n",
    "        for idx in occurance_out_index :\n",
    "            R_t.loc[idx,T_col] = t+window\n",
    "            R_t.loc[idx,E_col] = 0\n",
    "            \n",
    "        super_set = pd.concat([super_set,R_t],axis=0)\n",
    "        \n",
    "        # Leave only last measurements per each id & lm points\n",
    "        super_set = super_set.drop_duplicates([ID_col,'LM'],keep='last')\n",
    "        \n",
    "        # Time elapsed from measurement & LM time\n",
    "        super_set['diff'] = super_set['LM'] - super_set[measure_T_col]\n",
    "                \n",
    "    return  super_set.drop(columns = [measure_T_col], axis=1).reset_index(drop=True)\n",
    "\n",
    "## LM_transformer2(discretizer) - outputs Discretized landmarking dataset\n",
    "## input should be output from basic lm_transformer\n",
    "def LM_transformer2(df,ID_col, T_col,E_col,window,S,measure_T_col, k_bin, train=True) :\n",
    "    super_set = df\n",
    "    \n",
    "    discretized_set = pd.DataFrame()\n",
    "\n",
    "    for s in S :\n",
    "        temp = super_set[super_set['LM'] == s].reset_index(drop=True)\n",
    "        temp_bin = np.linspace(s, s+window, k_bin)\n",
    "\n",
    "        temp_digitize = np.digitize(temp[T_col],temp_bin, right =True)\n",
    "        temp['bin'] = temp_digitize    \n",
    "\n",
    "        \n",
    "        for i in range(temp.shape[0]) :\n",
    "            temp2 = temp.copy().iloc[i,:]\n",
    "            if train :\n",
    "                for j in range(1,temp_digitize[i]) :\n",
    "                    temp2['bin'] = j\n",
    "                    temp2[E_col] = 0\n",
    "                    discretized_set = pd.concat([discretized_set,temp2],axis=1)\n",
    "                    \n",
    "                temp2['bin'] = temp_digitize[i]\n",
    "                temp2[E_col] = temp.loc[i,E_col]\n",
    "                discretized_set = pd.concat([discretized_set,temp2],axis=1)\n",
    "                \n",
    "            else :\n",
    "                for j in range(1,k_bin) :\n",
    "                    temp2['bin'] = j\n",
    "                    temp2[E_col] = 0\n",
    "                    discretized_set = pd.concat([discretized_set,temp2],axis=1)\n",
    "                \n",
    "        \n",
    "    discretized_set = discretized_set.T\n",
    "    \n",
    "    return discretized_set.drop(columns = [T_col], axis=1).reset_index(drop=True)\n",
    "\n",
    "# Train-test split by ID, p is proportion of train set\n",
    "def splitID(data,ID_col,p) :\n",
    "    # Unique ID names\n",
    "    unique_ids = np.unique(data[ID_col])\n",
    "\n",
    "    # Number of samples within each train and test set\n",
    "    n_train = round(len(unique_ids)*0.7)\n",
    "    n_test = len(unique_ids) - n_train\n",
    "    \n",
    "    # IDs within train set and test set\n",
    "    train_ids = list(sample(set(unique_ids), n_train))\n",
    "    test_ids = list(set(unique_ids).difference(set(train_ids)))\n",
    "\n",
    "    # Row-wise masking for train and test set\n",
    "    mask_train = data[ID_col].isin(train_ids)\n",
    "    mask_test = data[ID_col].isin(test_ids)\n",
    "\n",
    "    # final train and test sets\n",
    "    data_train = data[mask_train].reset_index(drop=True)\n",
    "    data_test = data[mask_test].reset_index(drop=True)\n",
    "    \n",
    "    return data_train, data_test\n",
    "\n",
    "# boot_weight : outputs boostrapped sample from df\n",
    "# 'weight_boot' indicates how many times certain ID is selected in boostrapped sample\n",
    "def boot_weight(df, ID_col, boot=True) : \n",
    "    unique_ids = np.unique(df[ID_col])\n",
    "    \n",
    "    train_boot = np.random.choice(a = unique_ids, replace = boot, size =  len(unique_ids))\n",
    "    boot_counts = pd.DataFrame.from_dict(dict(Counter(train_boot)),orient='index').reset_index()\n",
    "    boot_counts.columns = [ID_col, 'weight_boot']\n",
    "    \n",
    "    return pd.merge(left=pd.DataFrame({ID_col : unique_ids}), right=boot_counts, how='left', on=ID_col).fillna(0)\n",
    "\n",
    "# kfold generator/iterator given ID \n",
    "# outputs kfold train and test sets.\n",
    "class kfold :\n",
    "    def __init__(self, k, ID_col, df1, df2, df3_train, df3_validation) :\n",
    "        self.k = k\n",
    "        self.ID_col = ID_col\n",
    "        self.df1 = df1\n",
    "        self.df2 = df2\n",
    "        self.df3_train = df3_train\n",
    "        self.df3_validation = df3_validation\n",
    "        \n",
    "        self.kf = KFold(n_splits=k, shuffle=True)\n",
    "        \n",
    "        # unique ids in b_th bootstrapped sample        \n",
    "        self.unique_ids = np.unique(df1[ID_col])\n",
    "        \n",
    "        # \n",
    "        self.k_fold = 0 \n",
    "        \n",
    "        \n",
    "        # where ids in each kth train set and validation set is stored \n",
    "        fold_train_id = []\n",
    "        fold_validation_id = []\n",
    "\n",
    "        for train_unique_id_idx, validation_unique_id_idx in self.kf.split(self.unique_ids) :\n",
    "            fold_train_id.append(self.unique_ids[train_unique_id_idx])\n",
    "            fold_validation_id.append(self.unique_ids[validation_unique_id_idx])\n",
    "        \n",
    "        self.fold_train_id = fold_train_id\n",
    "        self.fold_validation_id = fold_validation_id\n",
    "        \n",
    "        \n",
    "    def __iter__(self) : \n",
    "        return self\n",
    "    \n",
    "    def __next__(self) : \n",
    "        if self.k_fold > (self.k) :\n",
    "            raise StopIteration\n",
    "            \n",
    "        else :\n",
    "            # df1 - original dataset\n",
    "            mask1_train = self.df1[self.ID_col].isin(self.fold_train_id[self.k_fold])\n",
    "            mask1_validation = self.df1[self.ID_col].isin(self.fold_validation_id[self.k_fold])\n",
    "\n",
    "            df1_k_train = self.df1[mask1_train]\n",
    "            df1_k_validation = self.df1[mask1_validation]\n",
    "\n",
    "            # df2 - output of LM_transformer1 \n",
    "            mask2_k_train = self.df2[self.ID_col].isin(self.fold_train_id[self.k_fold])\n",
    "            mask2_k_validation = self.df2[self.ID_col].isin(self.fold_validation_id[self.k_fold])\n",
    "\n",
    "            df2_k_train = self.df2[mask2_k_train]\n",
    "            df2_k_validation = self.df2[mask2_k_validation]\n",
    "\n",
    "            # df3 - output of LM_transformer2\n",
    "            mask3_k_train = self.df3_train[self.ID_col].isin(self.fold_train_id[self.k_fold])\n",
    "            mask3_k_validation = self.df3_validation[self.ID_col].isin(self.fold_validation_id[self.k_fold])\n",
    "\n",
    "            df3_k_train = self.df3_train[mask3_k_train]\n",
    "            df3_k_validation = self.df3_validation[mask3_k_validation]\n",
    "            \n",
    "            self.k_fold += 1\n",
    "            print('$$$')\n",
    "            print('Iteration : ',self.k_fold)\n",
    "            return df1_k_train, df1_k_validation, df2_k_train, df2_k_validation, df3_k_train, df3_k_validation\n",
    "        \n",
    "def add_weight_column(train_df_list, ID_col, T_col, E_col, boot, S, window) :\n",
    "    \n",
    "    # add bootstrap weight & seperate inbag/outbag samples\n",
    "    boot_weight_at_b = boot_weight(df = train_df_list[0], ID_col = ID_col, boot=boot)\n",
    "\n",
    "    train_df_list_inbag = [];train_df_list_outbag = []\n",
    "    for df_temp in train_df_list :\n",
    "        df_inbag = pd.merge(pd.DataFrame.copy(df_temp), right = boot_weight_at_b, how='left', on= ID_col); df_inbag = df_inbag[df_inbag.weight_boot !=0]\n",
    "        df_outbag = pd.merge(pd.DataFrame.copy(df_temp), right = boot_weight_at_b, how='left', on= ID_col); df_outbag = df_outbag[df_outbag.weight_boot ==0]\n",
    "\n",
    "        train_df_list_inbag.append(df_inbag)\n",
    "        train_df_list_outbag.append(df_outbag)\n",
    "\n",
    "    # add IPC weight part\n",
    "    KM_cens = KaplanMeierFitter()\n",
    "    df_temp = train_df_list_inbag[0].drop_duplicates([ID_col])\n",
    "\n",
    "    cens_prob = []\n",
    "    for s in S :\n",
    "        df_risk = df_temp[df_temp[T_col]>s]\n",
    "        df_risk['LM'] = s\n",
    "        n_risk = df_risk.shape[0]\n",
    "\n",
    "        KM_cens.fit(durations = df_risk[T_col], event_observed = abs(df_risk[E_col]-1), weights  = df_risk['weight_boot'])\n",
    "\n",
    "        cens_prob.append(np.array(KM_cens.predict(train_df_list_inbag[1].loc[train_df_list_inbag[1].LM == s].sort_values(ID_col)[T_col]) + 10**(-10))*n_risk)\n",
    "\n",
    "    cens_prob = [item for sublist in cens_prob for item in sublist]\n",
    "    IPC_weight_at_b = train_df_list_inbag[1][[ID_col, 'LM',T_col,E_col]].sort_values(['LM',ID_col])\n",
    "    IPC_weight_at_b['cens_prob'] = cens_prob; IPC_weight_at_b['weight_IPC'] = 1/IPC_weight_at_b['cens_prob']\n",
    "    IPC_weight_at_b.loc[((IPC_weight_at_b[T_col] < IPC_weight_at_b['LM']+window)&(IPC_weight_at_b[E_col]==0)),'weight_IPC'] = 0 \n",
    "\n",
    "    IPC_weight_at_b = IPC_weight_at_b[[ID_col, 'LM', 'weight_IPC']]\n",
    "    \n",
    "    for i in range(1,len(train_df_list_inbag)) :\n",
    "        train_df_list_inbag[i] = train_df_list_inbag[i].merge(IPC_weight_at_b,how='left', on = [ID_col, 'LM'])\n",
    "        train_df_list_inbag[i]['weight'] = train_df_list_inbag[i]['weight_boot']*train_df_list_inbag[i]['weight_IPC']*(10**4) + 10**(-10)\n",
    "        train_df_list_inbag[i] = train_df_list_inbag[i].drop(['weight_boot','weight_IPC'],axis=1)\n",
    "    \n",
    "    return(train_df_list_inbag, train_df_list_outbag)\n",
    "\n",
    "\n",
    "def v_year_survival_prob_cox(model, ID_col, test_set, S ,window) :\n",
    "    # predict survival probability in each time grid (given LM points)\n",
    "    predicted_survival = model.predict_survival_function(test_set.drop(ID_col, axis=1), times= S + window)\n",
    "    \n",
    "    # discretized survival probability from each LM points to LM points + window(v)\n",
    "    time = test_set.LM + window\n",
    "\n",
    "    v_year_surv_prob = []\n",
    "    for idx in time.index : \n",
    "        value = predicted_survival.loc[time[idx],idx]\n",
    "        v_year_surv_prob.append(value)\n",
    "    return np.array(v_year_surv_prob)\n",
    "\n",
    "\n",
    "def v_year_survival_prob_ml(model, ID_col, E_col, test_set) :\n",
    "    del_col = [col for col in test_set.columns if \"weight\" in col]; del_col.append(ID_col) ; del_col.append(E_col)\n",
    "    surv_prob = pd.DataFrame(model.predict_proba(test_set.drop(del_col, axis=1))[:,0])\n",
    "\n",
    "    output = pd.concat([test_set[[ID_col, 'LM', 'bin']].reset_index(drop = True), surv_prob],axis=1)\n",
    "    output = output.pivot_table(index=['LM',ID_col], columns='bin', values=0)\n",
    "\n",
    "    output = output.reset_index(drop=True)\n",
    "    # cumprod from column 3(surv_1) ~ surv_last\n",
    "    output = np.cumprod(output,axis= 1)\n",
    "    \n",
    "    return output.iloc[:,-1]\n",
    "\n",
    "\n",
    "def level_1_stack(model_specifics_1,ID_col, E_col, T_col, measure_T_col, window, S, k_bin, \n",
    "                  train_sets, validation_sets) :\n",
    "\n",
    "    true_survival_status = np.array(1 - np.array(validation_sets[1][E_col]))\n",
    "    \n",
    "    out = true_survival_status\n",
    "    model_specifics = model_specifics_1.reset_index(drop = True)\n",
    "    \n",
    "    for g_1 in range(model_specifics.shape[0]) : \n",
    "        model_name = model_specifics.loc[g_1,'model_name'] \n",
    "        model_instance = model_specifics.loc[g_1,'model_instance']\n",
    "        model_hyperparams = model_specifics.loc[g_1,'hyperparams']\n",
    "        model_type = model_specifics.loc[g_1,'type']\n",
    "        \n",
    "        print(model_name)\n",
    "\n",
    "        param_combinations = list(itertools.product(*list(model_hyperparams.values())))\n",
    "        param_names = list(model_hyperparams.keys())\n",
    "\n",
    "        if model_type == 'cont' : # Cox model\n",
    "            # feed appropriate form of train validation data\n",
    "            train_data = train_sets[1]\n",
    "            validation_data = validation_sets[1]\n",
    "            \n",
    "            # change hyperparameters according to model_hyperparameter grid\n",
    "            for g_2 in range(len(param_combinations)) :\n",
    "                for param_idx in range(len(param_names)) :\n",
    "                    setattr(model_instance, param_names[param_idx], param_combinations[g_2][param_idx])\n",
    "                \n",
    "                model_instance.fit(df = train_data.drop([ID_col],axis=1), duration_col = T_col, event_col = E_col,weights_col = 'weight' ,step_size = 0.01, robust=True)\n",
    "                # print(model_instance.print_summary())\n",
    "\n",
    "                surv_prob_est = v_year_survival_prob_cox(model = model_instance,ID_col= ID_col ,test_set = validation_data, S=S ,window = window)\n",
    "                out = np.c_[out, surv_prob_est]\n",
    "\n",
    "        elif model_type == 'disc' : \n",
    "            # feed appropriate form of train validation data\n",
    "            train_data = train_sets[2]\n",
    "            validation_data = validation_sets[2]\n",
    "            \n",
    "            # change hyperparameters according to model_hyperparameter grid\n",
    "            for g_2 in range(len(param_combinations)) :\n",
    "                for param_idx in range(len(param_names)) :\n",
    "                    setattr(model_instance, param_names[param_idx], param_combinations[g_2][param_idx])\n",
    "                \n",
    "                if model_name in ['KNN','MLP'] : \n",
    "                    model_instance.fit(train_data.drop([ID_col, E_col,'weight'],axis=1),train_data[E_col])                    \n",
    "                else :     \n",
    "                    model_instance.fit(train_data.drop([ID_col, E_col,'weight'],axis=1),train_data[E_col], train_data['weight'])\n",
    "\n",
    "\n",
    "                surv_prob_est = v_year_survival_prob_ml(model = model_instance, ID_col = ID_col, E_col = E_col, test_set = validation_data)\n",
    "                out = np.c_[out, surv_prob_est]\n",
    "        \n",
    "    # out : first column in true value, \n",
    "    #       2nd column to end is predicted survival prob from each models with different hyperparam settings\n",
    "    return out\n",
    "\n",
    "class nnls_constraint() : \n",
    "    def __init__(self, tol = 10**(-5), max_iter = 10^5) : \n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def fit(self, x, y, w) : \n",
    "        n, k = x.shape\n",
    "        obj = lambda beta, y, x, w : np.dot(w.reshape(-1,), (np.array(y).reshape(-1, ) - x @ beta)**2)/n\n",
    "        \n",
    "        # bound(0-1) and constrant(beta sum to 1)\n",
    "        bnds = list(tuple(itertools.repeat((0,1),k)))\n",
    "        cons = [{\"type\": \"eq\", \"fun\": lambda beta: np.sum(beta) - 1}]\n",
    "\n",
    "        # Initial guess for betas\n",
    "        init = np.repeat(0,k)\n",
    "        \n",
    "        # minimization\n",
    "        res = minimize(obj, args=(y, x, w), x0=init, bounds=bnds, constraints=cons, tol = self.tol, options= {'maxiter':self.max_iter})\n",
    "        \n",
    "        self.coef_ = res.x\n",
    "        self.iter = res['nit']\n",
    "        self.score = res['fun']\n",
    "        self.res = res\n",
    "        \n",
    "        return \n",
    "\n",
    "    def predict(self, x) : \n",
    "        return x @ self.coef_\n",
    "        \n",
    "\n",
    "class hillclimb() : \n",
    "    def __init__(self, max_iter= 2000, early_stop_n = 50, early_stop_eps = 10**(-3)) : \n",
    "        self.max_iter = max_iter\n",
    "        self.early_stop_n = early_stop_n\n",
    "        self.early_stop_eps = early_stop_eps\n",
    "        return\n",
    "        \n",
    "    def fit(self, x, y, w) : \n",
    "        n, k = x.shape\n",
    "        coef_ = np.zeros(k)\n",
    "        \n",
    "        current_score = 10^10\n",
    "        \n",
    "        current_iter = 0; early_stop_iter = 0 \n",
    "        while (current_iter <= self.max_iter)&(early_stop_iter <= self.early_stop_n) :\n",
    "            \n",
    "            # search\n",
    "            next_scores = []\n",
    "            for i in range(k) : \n",
    "                temp_coef_ = copy.copy(coef_); temp_coef_[i] += 1\n",
    "                temp_score = brier_score_loss(y, x @ (temp_coef_ / sum(temp_coef_)),w)\n",
    "                next_scores.append(temp_score)\n",
    "            \n",
    "            \n",
    "            # update\n",
    "            next_score = min(next_scores)\n",
    "            \n",
    "            best_ind = next_scores.index(next_score)\n",
    "            coef_[best_ind] = coef_[best_ind]+1\n",
    "            \n",
    "            current_iter += 1\n",
    "            \n",
    "            if (current_score - next_score) > self.early_stop_eps :\n",
    "                early_stop_iter = 0 \n",
    "            else : \n",
    "                early_stop_iter += 1\n",
    "            \n",
    "            current_score = next_score\n",
    "        \n",
    "        self.coef_ = coef_ / sum(coef_)\n",
    "        self.iter = current_iter    \n",
    "        self.score = current_score\n",
    "            \n",
    "    def predict(self, x) : \n",
    "        return x @ self.coef_\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492cbdf5-4228-4f5d-a1ba-14353cfe6a6e",
   "metadata": {},
   "source": [
    "# Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd0cf07-2cc7-452d-ba22-7b0011308762",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "# loading data & preprop\n",
    "\n",
    "# settings \n",
    "dir = \"/Users/pio/Google 드라이브/data/\"\n",
    "file_name = \"pbc2.csv\"\n",
    "data = pd.read_csv(dir + file_name)\n",
    "\n",
    "# drop status1 - competing risks setting\n",
    "data = data.drop(axis=1, columns =['status'])\n",
    "\n",
    "\n",
    "# ID, Time, Event, Measure Time column names\n",
    "ID_col = 'id'; T_col ='years'; E_col ='status2'; measure_T_col = 'year'\n",
    "\n",
    "# categorical variables\n",
    "nominal_col = ['drug','sex', 'ascites', 'hepatomegaly','spiders', 'edema']\n",
    "ordinal_col = ['histologic']\n",
    "\n",
    "# continuous variables\n",
    "cont_col = list(set(data.columns) - set(nominal_col) - set(ordinal_col) - set([ID_col, T_col, E_col, measure_T_col]))\n",
    "\n",
    "# window - 5 year prediction \n",
    "window = 5\n",
    "\n",
    "# S : landmark time points - 0, 0.5, 1, ..., 10\n",
    "S = np.linspace(0,10,21)\n",
    "v_years = S+window\n",
    "\n",
    "# Number of bins when discritizing \n",
    "## !!!(Actually, k_bin - 1 bins are produced)!!!\n",
    "k_bin = 5\n",
    "\n",
    "# minimal bin_size\n",
    "minimal_bin_size = window / (k_bin-1)\n",
    "# t_grid -> minimal points where survival probabilities are measured\n",
    "# t_grid = np.arange(0,S[-1] + window + minimal_bin_size, step = minimal_bin_size)\n",
    "\n",
    "# imputation -> fill na's : median for continous\n",
    "for col in cont_col : \n",
    "    data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "\n",
    "# one-hot encoding for categorical variables\n",
    "data = pd.get_dummies(data, columns = nominal_col, drop_first=True)\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "# settings2\n",
    "\n",
    "# proportion of train set\n",
    "p_train = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfc33a1f-a516-4085-af4d-cc381e23910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "feature_cols = ['age','serBilir', 'serChol', 'albumin','alkaline', 'SGOT', 'platelets', 'prothrombin', 'histologic', 'status2','drug_placebo', 'sex_male', 'ascites_Yes', 'hepatomegaly_Yes',\n",
    "'spiders_Yes', 'edema_edema despite diuretics','edema_edema no diuretics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c8f323a-70fa-4c96-a668-bbb017852ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1344, 20)\n",
      "(601, 20)\n",
      "seed : 0\n",
      "Intersection :  set()\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "random.seed(i)\n",
    "train, test = splitID(data = data,ID_col = ID_col, p = p_train)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('seed : '+ str(i))\n",
    "print('Intersection : ', set(np.unique(train[ID_col])).intersection(set(np.unique(test[ID_col]))))\n",
    "\n",
    "train[feature_cols] = scaler.fit_transform(train[feature_cols])\n",
    "test[feature_cols] = scaler.transform(test[feature_cols])\n",
    "\n",
    "\n",
    "train_lm1 = LM_transformer(df=train,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col)\n",
    "test_lm1 = LM_transformer(df=test,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col)\n",
    "\n",
    "train_lm2_train_ver = LM_transformer2(df=train_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=True)\n",
    "train_lm2_validation_ver = LM_transformer2(df=train_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=False)\n",
    "\n",
    "test_lm2 = LM_transformer2(df=test_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=False)\n",
    "\n",
    "# write file\n",
    "train.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train'+'.csv',index=False)\n",
    "test.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_test'+'.csv',index=False)\n",
    "\n",
    "train_lm1.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train_lm1'+'.csv',index=False)\n",
    "test_lm1.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_test_lm1'+'.csv',index=False)\n",
    "\n",
    "train_lm2_train_ver.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train_lm2_train_ver'+'.csv',index=False)\n",
    "train_lm2_validation_ver.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_train_lm2_validation_ver'+'.csv',index=False)\n",
    "test_lm2.to_csv('/Users/pio/Google 드라이브/github/survival ensemble/dataset/'+'pbc2_seed_'+str(i)+'_test_lm2'+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe0a5004-4c4d-4f79-a5d5-63636960135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting : \n",
    "\n",
    "# B : number of resampling / K : number of folds / boot : replacement true false\n",
    "B = 1; K = 3; boot = False\n",
    "\n",
    "base_info = {'ID_col':ID_col, 'T_col':T_col, 'E_col':E_col, 'measure_T_col':measure_T_col, 'boot':boot, 'B':B, 'K':K, \n",
    "            'window':window , 'S' :S, 'k_bin':k_bin}\n",
    "\n",
    "# model specifics : model name & model instance & hyperparameter grid & type of model\n",
    "## type of model : cont(continous) or disc(discrete)\n",
    "\n",
    "## model specifics of level 1 models\n",
    "cox1_params = {'penalizer':[0,0.05,0.1,0.5],'l1_ratio':[0,0.25,0.5,0.75,1]}\n",
    "\n",
    "model_specifics_cont = pd.DataFrame({'model_name' : ['cox1'], \n",
    "                                'model_instance':[CoxPHFitter()], \n",
    "                                'hyperparams':[cox1_params], \n",
    "                                'type':['cont']})\n",
    "\n",
    "LR_params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga']\n",
    "}\n",
    "RF_params = {'n_estimators':[50,100,300,500],'max_depth':[1,3,5]}\n",
    "GB_params = {'n_estimators':[50,100,300,500],'max_depth':[1,3,5]}\n",
    "MLP_params = {'hidden_layer_sizes':[1,2,3], 'activation' : ['identity', 'logistic', 'tanh', 'relu'], 'max_iter' : [1000], 'early_stopping' : [True], 'learning_rate' : ['adaptive']}\n",
    "KNN_params = {'n_neighbors':[1,5,10], 'weights':['uniform', 'distance']}\n",
    "NGB_params = {'var_smoothing':[1e-5, 1e-9, 1e-1]}\n",
    "ADA_params = {'n_estimators':[50, 100, 300, 500], 'learning_rate':np.linspace(0.1,2,10), 'max_depth':[1,3,5]}\n",
    "\n",
    "\n",
    "model_specifics_disc = pd.DataFrame({'model_name' : ['LR','RF','GB','MLP','KNN','NGB','ADA'], \n",
    "                                'model_instance':[LogisticRegression(max_iter=10000),RandomForestClassifier(),GradientBoostingClassifier(),MLPClassifier(),KNeighborsClassifier(),GaussianNB(), AdaBoostClassifier()], \n",
    "                                'hyperparams':[LR_params, RF_params, GB_params,MLP_params, KNN_params,NGB_params, ADA_params], \n",
    "                                'type':['disc','disc','disc','disc','disc','disc','disc']})\n",
    "\n",
    "\n",
    "model_specifics_1 = pd.concat([model_specifics_cont,model_specifics_disc],axis=0).reset_index(drop=True)\n",
    "\n",
    "## model specifics of level 2 models\n",
    "model_specifics_2 = pd.DataFrame({'model_name':['M1'], \n",
    "                                  'model_instance':[LogisticRegression(max_iter=10000)],\n",
    "                                  'hyperparams':[{'C':[0.05, 10]}],\n",
    "                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08f99957-68bf-4f58-9fb8-21d243ef0afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_instance</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cox1</td>\n",
       "      <td>&lt;lifelines.CoxPHFitter&gt;</td>\n",
       "      <td>{'penalizer': [0, 0.05, 0.1, 0.5], 'l1_ratio':...</td>\n",
       "      <td>cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'p...</td>\n",
       "      <td>disc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>{'n_estimators': [50, 100, 300, 500], 'max_dep...</td>\n",
       "      <td>disc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier()</td>\n",
       "      <td>{'n_estimators': [50, 100, 300, 500], 'max_dep...</td>\n",
       "      <td>disc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>{'hidden_layer_sizes': [1, 2, 3], 'activation'...</td>\n",
       "      <td>disc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>{'n_neighbors': [1, 5, 10], 'weights': ['unifo...</td>\n",
       "      <td>disc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NGB</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>{'var_smoothing': [1e-05, 1e-09, 0.1]}</td>\n",
       "      <td>disc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ADA</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>{'n_estimators': [10, 50, 100, 250, 500], 'lea...</td>\n",
       "      <td>disc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name                      model_instance  \\\n",
       "0       cox1             <lifelines.CoxPHFitter>   \n",
       "1         LR  LogisticRegression(max_iter=10000)   \n",
       "2         RF            RandomForestClassifier()   \n",
       "3         GB        GradientBoostingClassifier()   \n",
       "4        MLP                     MLPClassifier()   \n",
       "5        KNN              KNeighborsClassifier()   \n",
       "6        NGB                        GaussianNB()   \n",
       "7        ADA                AdaBoostClassifier()   \n",
       "\n",
       "                                         hyperparams  type  \n",
       "0  {'penalizer': [0, 0.05, 0.1, 0.5], 'l1_ratio':...  cont  \n",
       "1  {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'p...  disc  \n",
       "2  {'n_estimators': [50, 100, 300, 500], 'max_dep...  disc  \n",
       "3  {'n_estimators': [50, 100, 300, 500], 'max_dep...  disc  \n",
       "4  {'hidden_layer_sizes': [1, 2, 3], 'activation'...  disc  \n",
       "5  {'n_neighbors': [1, 5, 10], 'weights': ['unifo...  disc  \n",
       "6             {'var_smoothing': [1e-05, 1e-09, 0.1]}  disc  \n",
       "7  {'n_estimators': [10, 50, 100, 250, 500], 'lea...  disc  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_specifics_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b07667a-f537-4905-94bf-38cb64ef8121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "1 / 1  Resampled\n",
      "1 / 3  fold\n",
      "$$$\n",
      "Iteration :  1\n",
      "cox1\n",
      "LR\n",
      "RF\n",
      "GB\n",
      "MLP\n",
      "KNN\n",
      "NGB\n",
      "ADA\n",
      "2 / 3  fold\n",
      "$$$\n",
      "Iteration :  2\n",
      "cox1\n",
      "LR\n",
      "RF\n",
      "GB\n",
      "MLP\n",
      "KNN\n",
      "NGB\n",
      "ADA\n",
      "3 / 3  fold\n",
      "$$$\n",
      "Iteration :  3\n",
      "cox1\n",
      "LR\n",
      "RF\n",
      "GB\n",
      "MLP\n",
      "KNN\n",
      "NGB\n",
      "ADA\n",
      "cox1\n",
      "LR\n",
      "RF\n",
      "GB\n",
      "MLP\n",
      "KNN\n",
      "NGB\n",
      "ADA\n"
     ]
    }
   ],
   "source": [
    "dir_temp = '/Users/pio/Google 드라이브/github/survival ensemble/dataset/pbc2'\n",
    "\n",
    "# Read ith dataset \n",
    "# directory of ith sets\n",
    "i = 0\n",
    "train_dir = dir_temp+'_seed_'+str(i)+'_'+'train.csv'\n",
    "test_dir = dir_temp+'_seed_'+str(i)+'_'+'test.csv'\n",
    "\n",
    "train_lm1_dir = dir_temp+'_seed_'+str(i)+'_'+'train_lm1.csv'\n",
    "test_lm1_dir = dir_temp+'_seed_'+str(i)+'_'+'test_lm1.csv'\n",
    "\n",
    "train_lm2_train_ver_dir = dir_temp+'_seed_'+str(i)+'_'+'train_lm2_train_ver.csv'\n",
    "train_lm2_validation_ver_dir = dir_temp+'_seed_'+str(i)+'_'+'train_lm2_validation_ver.csv'\n",
    "test_lm2_dir = dir_temp+'_seed_'+str(i)+'_'+'test_lm2.csv'\n",
    "\n",
    "# read ith sets\n",
    "train = pd.read_csv(train_dir)\n",
    "test = pd.read_csv(test_dir)\n",
    "\n",
    "train_lm1 = pd.read_csv(train_lm1_dir)\n",
    "test_lm1 = pd.read_csv(test_lm1_dir)\n",
    "\n",
    "train_lm2_train_ver = pd.read_csv(train_lm2_train_ver_dir)\n",
    "train_lm2_validation_ver = pd.read_csv(train_lm2_validation_ver_dir)\n",
    "test_lm2 = pd.read_csv(test_lm2_dir)\n",
    "\n",
    "# super set(stacking)\n",
    "\n",
    "train_df_list = [train, train_lm1, train_lm2_train_ver, train_lm2_validation_ver]\n",
    "test_df_list = [test, test_lm1, test_lm2]\n",
    "\n",
    "\n",
    "stacked_noboot = boot_kfold(base_info = base_info, train_df_list = train_df_list, \n",
    "       test_df_list = test_df_list,\n",
    "       model_specifics_1 = model_specifics_1, \n",
    "       model_specifics_2 = model_specifics_2)\n",
    "\n",
    "\n",
    "stacked_noboot.boot_stack()\n",
    "\n",
    "\n",
    "# store supersets\n",
    "pd.DataFrame(stacked_noboot.train_supersets[0][:,1:]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'train_stack_X.csv',index=False)\n",
    "pd.DataFrame(stacked_noboot.train_supersets[0][:,0]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'train_stack_y.csv',index=False)\n",
    "pd.DataFrame(stacked_noboot.weights[0]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'train_stack_w.csv',index=False)\n",
    "\n",
    "pd.DataFrame(stacked_noboot.test_superset[0][:,1:]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'test_stack_X.csv',index=False)\n",
    "pd.DataFrame(stacked_noboot.test_superset[0][:,0]).to_csv(dir_temp+'_seed_'+str(i)+'_'+'test_stack_y.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2913b8cb-80fb-41aa-bd3c-8403b152c8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>serBilir</th>\n",
       "      <th>serChol</th>\n",
       "      <th>albumin</th>\n",
       "      <th>alkaline</th>\n",
       "      <th>SGOT</th>\n",
       "      <th>platelets</th>\n",
       "      <th>prothrombin</th>\n",
       "      <th>histologic</th>\n",
       "      <th>...</th>\n",
       "      <th>drug_placebo</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>ascites_Yes</th>\n",
       "      <th>hepatomegaly_Yes</th>\n",
       "      <th>spiders_Yes</th>\n",
       "      <th>edema_edema despite diuretics</th>\n",
       "      <th>edema_edema no diuretics</th>\n",
       "      <th>LM</th>\n",
       "      <th>diff</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.622822</td>\n",
       "      <td>0.350490</td>\n",
       "      <td>0.124321</td>\n",
       "      <td>0.209064</td>\n",
       "      <td>0.115642</td>\n",
       "      <td>0.109943</td>\n",
       "      <td>0.156842</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.578364</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.529056</td>\n",
       "      <td>0.089506</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.578364</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.529056</td>\n",
       "      <td>0.089506</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.578364</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.529056</td>\n",
       "      <td>0.089506</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.578364</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.529056</td>\n",
       "      <td>0.089506</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8352</th>\n",
       "      <td>134.0</td>\n",
       "      <td>0.307842</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.252867</td>\n",
       "      <td>0.244152</td>\n",
       "      <td>0.178415</td>\n",
       "      <td>0.128295</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.113350</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8353</th>\n",
       "      <td>135.0</td>\n",
       "      <td>0.319966</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.121907</td>\n",
       "      <td>0.267544</td>\n",
       "      <td>0.031387</td>\n",
       "      <td>0.026527</td>\n",
       "      <td>0.386316</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.356368</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8354</th>\n",
       "      <td>137.0</td>\n",
       "      <td>0.701344</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.173808</td>\n",
       "      <td>0.269006</td>\n",
       "      <td>0.055855</td>\n",
       "      <td>0.050717</td>\n",
       "      <td>0.285263</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.515825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8355</th>\n",
       "      <td>140.0</td>\n",
       "      <td>0.532910</td>\n",
       "      <td>0.017157</td>\n",
       "      <td>0.132167</td>\n",
       "      <td>0.349415</td>\n",
       "      <td>0.049665</td>\n",
       "      <td>0.058225</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.165371</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8356</th>\n",
       "      <td>141.0</td>\n",
       "      <td>0.397963</td>\n",
       "      <td>0.012255</td>\n",
       "      <td>0.136391</td>\n",
       "      <td>0.340643</td>\n",
       "      <td>0.046534</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>0.273684</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.973278</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8357 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       age  serBilir   serChol   albumin  alkaline      SGOT  \\\n",
       "0       1.0  0.622822  0.350490  0.124321  0.209064  0.115642  0.109943   \n",
       "1       2.0  0.578364  0.022059  0.149065  0.434211  0.529056  0.089506   \n",
       "2       2.0  0.578364  0.022059  0.149065  0.434211  0.529056  0.089506   \n",
       "3       2.0  0.578364  0.022059  0.149065  0.434211  0.529056  0.089506   \n",
       "4       2.0  0.578364  0.022059  0.149065  0.434211  0.529056  0.089506   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "8352  134.0  0.307842  0.125000  0.252867  0.244152  0.178415  0.128295   \n",
       "8353  135.0  0.319966  0.007353  0.121907  0.267544  0.031387  0.026527   \n",
       "8354  137.0  0.701344  0.009804  0.173808  0.269006  0.055855  0.050717   \n",
       "8355  140.0  0.532910  0.017157  0.132167  0.349415  0.049665  0.058225   \n",
       "8356  141.0  0.397963  0.012255  0.136391  0.340643  0.046534  0.086670   \n",
       "\n",
       "      platelets  prothrombin  histologic  ...  drug_placebo  sex_male  \\\n",
       "0      0.156842     0.266667    1.000000  ...           0.0       0.0   \n",
       "1      0.189474     0.133333    0.666667  ...           0.0       0.0   \n",
       "2      0.189474     0.133333    0.666667  ...           0.0       0.0   \n",
       "3      0.189474     0.133333    0.666667  ...           0.0       0.0   \n",
       "4      0.189474     0.133333    0.666667  ...           0.0       0.0   \n",
       "...         ...          ...         ...  ...           ...       ...   \n",
       "8352   0.136842     0.316667    1.000000  ...           1.0       0.0   \n",
       "8353   0.386316     0.183333    0.333333  ...           0.0       0.0   \n",
       "8354   0.285263     0.200000    0.666667  ...           0.0       0.0   \n",
       "8355   0.200000     0.216667    1.000000  ...           0.0       0.0   \n",
       "8356   0.273684     0.133333    0.333333  ...           0.0       0.0   \n",
       "\n",
       "      ascites_Yes  hepatomegaly_Yes  spiders_Yes  \\\n",
       "0             1.0               1.0          1.0   \n",
       "1             0.0               1.0          1.0   \n",
       "2             0.0               1.0          1.0   \n",
       "3             0.0               1.0          1.0   \n",
       "4             0.0               1.0          1.0   \n",
       "...           ...               ...          ...   \n",
       "8352          0.0               1.0          0.0   \n",
       "8353          0.0               0.0          0.0   \n",
       "8354          0.0               1.0          0.0   \n",
       "8355          0.0               1.0          0.0   \n",
       "8356          0.0               0.0          0.0   \n",
       "\n",
       "      edema_edema despite diuretics  edema_edema no diuretics    LM      diff  \\\n",
       "0                               1.0                       0.0   0.0  0.000000   \n",
       "1                               0.0                       0.0   0.0  0.000000   \n",
       "2                               0.0                       0.0   0.0  0.000000   \n",
       "3                               0.0                       0.0   0.0  0.000000   \n",
       "4                               0.0                       0.0   0.0  0.000000   \n",
       "...                             ...                       ...   ...       ...   \n",
       "8352                            0.0                       0.0  10.0  0.113350   \n",
       "8353                            0.0                       0.0  10.0  1.356368   \n",
       "8354                            0.0                       0.0  10.0  0.515825   \n",
       "8355                            0.0                       0.0  10.0  0.165371   \n",
       "8356                            0.0                       0.0  10.0  8.973278   \n",
       "\n",
       "      bin  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     2.0  \n",
       "3     3.0  \n",
       "4     4.0  \n",
       "...   ...  \n",
       "8352  1.0  \n",
       "8353  1.0  \n",
       "8354  1.0  \n",
       "8355  1.0  \n",
       "8356  1.0  \n",
       "\n",
       "[8357 rows x 21 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lm2_train_ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4cea0ba-f0d0-4263-8d3b-a957f073334f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[       id      years       age      year  serBilir   serChol   albumin  \\\n",
       "  0       1   1.095170  0.622822  0.000000  0.350490  0.124321  0.209064   \n",
       "  1       1   1.095170  0.622822  0.525682  0.517157  0.136391  0.258772   \n",
       "  2       2  14.152338  0.578364  0.000000  0.022059  0.149065  0.434211   \n",
       "  3       2  14.152338  0.578364  0.498302  0.014706  0.136391  0.355263   \n",
       "  4       2  14.152338  0.578364  0.999343  0.019608  0.136391  0.347953   \n",
       "  ...   ...        ...       ...       ...       ...       ...       ...   \n",
       "  1339  312   3.989158  0.131797  0.000000  0.151961  0.314424  0.383041   \n",
       "  1340  312   3.989158  0.131797  0.564013  0.129902  0.136391  0.296784   \n",
       "  1341  312   3.989158  0.131797  1.067791  0.176471  0.155100  0.349415   \n",
       "  1342  312   3.989158  0.131797  2.121892  0.394608  0.382016  0.317251   \n",
       "  1343  312   3.989158  0.131797  2.943270  0.568627  0.414001  0.328947   \n",
       "  \n",
       "        alkaline      SGOT  platelets  ...  histologic  status2  drug_placebo  \\\n",
       "  0     0.115642  0.109943   0.156842  ...    1.000000      1.0           0.0   \n",
       "  1     0.107923  0.000000   0.149474  ...    1.000000      1.0           0.0   \n",
       "  2     0.529056  0.089506   0.189474  ...    0.666667      0.0           0.0   \n",
       "  3     0.143970  0.111195   0.154737  ...    0.666667      0.0           0.0   \n",
       "  4     0.115133  0.115115   0.126316  ...    0.666667      0.0           0.0   \n",
       "  ...        ...       ...        ...  ...         ...      ...           ...   \n",
       "  1339  0.144553  0.108275   0.167368  ...    0.333333      0.0           1.0   \n",
       "  1340  0.112729  0.098265   0.155789  ...    0.333333      0.0           1.0   \n",
       "  1341  0.119211  0.133300   0.112632  ...    0.333333      0.0           1.0   \n",
       "  1342  0.169677  0.139139   0.102105  ...    0.333333      0.0           1.0   \n",
       "  1343  0.209875  0.161662   0.091579  ...    0.666667      0.0           1.0   \n",
       "  \n",
       "        sex_male  ascites_Yes  hepatomegaly_Yes  spiders_Yes  \\\n",
       "  0          0.0          1.0               1.0          1.0   \n",
       "  1          0.0          1.0               1.0          1.0   \n",
       "  2          0.0          0.0               1.0          1.0   \n",
       "  3          0.0          0.0               1.0          1.0   \n",
       "  4          0.0          0.0               1.0          1.0   \n",
       "  ...        ...          ...               ...          ...   \n",
       "  1339       0.0          0.0               0.0          1.0   \n",
       "  1340       0.0          0.0               0.0          0.0   \n",
       "  1341       0.0          0.0               0.0          0.0   \n",
       "  1342       0.0          0.0               0.0          1.0   \n",
       "  1343       0.0          0.0               0.0          1.0   \n",
       "  \n",
       "        edema_edema despite diuretics  edema_edema no diuretics  weight_boot  \n",
       "  0                               1.0                       0.0            1  \n",
       "  1                               1.0                       0.0            1  \n",
       "  2                               0.0                       0.0            1  \n",
       "  3                               0.0                       0.0            1  \n",
       "  4                               0.0                       0.0            1  \n",
       "  ...                             ...                       ...          ...  \n",
       "  1339                            0.0                       0.0            1  \n",
       "  1340                            0.0                       0.0            1  \n",
       "  1341                            0.0                       0.0            1  \n",
       "  1342                            0.0                       1.0            1  \n",
       "  1343                            0.0                       1.0            1  \n",
       "  \n",
       "  [1344 rows x 21 columns],\n",
       "         id      years       age  serBilir   serChol   albumin  alkaline  \\\n",
       "  0       1   1.095170  0.622822  0.350490  0.124321  0.209064  0.115642   \n",
       "  1       2   5.000000  0.578364  0.022059  0.149065  0.434211  0.529056   \n",
       "  2       5   4.120578  0.226748  0.078431  0.135184  0.345029  0.039397   \n",
       "  3       6   5.000000  0.766481  0.014706  0.116476  0.410819  0.059278   \n",
       "  4       7   5.000000  0.560886  0.019608  0.161135  0.426901  0.050539   \n",
       "  ...   ...        ...       ...       ...       ...       ...       ...   \n",
       "  2779  134  10.453401  0.307842  0.125000  0.252867  0.244152  0.178415   \n",
       "  2780  135  10.456138  0.319966  0.007353  0.121907  0.267544  0.031387   \n",
       "  2781  137  10.018070  0.701344  0.009804  0.173808  0.269006  0.055855   \n",
       "  2782  140  10.206987  0.532910  0.017157  0.132167  0.349415  0.049665   \n",
       "  2783  141  10.182346  0.397963  0.012255  0.136391  0.340643  0.046534   \n",
       "  \n",
       "            SGOT  platelets  prothrombin  ...  drug_placebo  sex_male  \\\n",
       "  0     0.109943   0.156842     0.266667  ...           0.0       0.0   \n",
       "  1     0.089506   0.189474     0.133333  ...           0.0       0.0   \n",
       "  2     0.089256   0.100000     0.158333  ...           1.0       0.0   \n",
       "  3     0.072406   0.196842     0.166667  ...           1.0       0.0   \n",
       "  4     0.045295   0.171579     0.058333  ...           1.0       0.0   \n",
       "  ...        ...        ...          ...  ...           ...       ...   \n",
       "  2779  0.128295   0.136842     0.316667  ...           1.0       0.0   \n",
       "  2780  0.026527   0.386316     0.183333  ...           0.0       0.0   \n",
       "  2781  0.050717   0.285263     0.200000  ...           0.0       0.0   \n",
       "  2782  0.058225   0.200000     0.216667  ...           0.0       0.0   \n",
       "  2783  0.086670   0.273684     0.133333  ...           0.0       0.0   \n",
       "  \n",
       "        ascites_Yes  hepatomegaly_Yes  spiders_Yes  \\\n",
       "  0             1.0               1.0          1.0   \n",
       "  1             0.0               1.0          1.0   \n",
       "  2             0.0               1.0          1.0   \n",
       "  3             0.0               1.0          0.0   \n",
       "  4             0.0               1.0          0.0   \n",
       "  ...           ...               ...          ...   \n",
       "  2779          0.0               1.0          0.0   \n",
       "  2780          0.0               0.0          0.0   \n",
       "  2781          0.0               1.0          0.0   \n",
       "  2782          0.0               1.0          0.0   \n",
       "  2783          0.0               0.0          0.0   \n",
       "  \n",
       "        edema_edema despite diuretics  edema_edema no diuretics    LM      diff  \\\n",
       "  0                               1.0                       0.0   0.0  0.000000   \n",
       "  1                               0.0                       0.0   0.0  0.000000   \n",
       "  2                               0.0                       0.0   0.0  0.000000   \n",
       "  3                               0.0                       0.0   0.0  0.000000   \n",
       "  4                               0.0                       0.0   0.0  0.000000   \n",
       "  ...                             ...                       ...   ...       ...   \n",
       "  2779                            0.0                       0.0  10.0  0.113350   \n",
       "  2780                            0.0                       0.0  10.0  1.356368   \n",
       "  2781                            0.0                       0.0  10.0  0.515825   \n",
       "  2782                            0.0                       0.0  10.0  0.165371   \n",
       "  2783                            0.0                       0.0  10.0  8.973278   \n",
       "  \n",
       "              weight  \n",
       "  0     4.587156e+01  \n",
       "  1     5.092195e+01  \n",
       "  2     1.000000e-10  \n",
       "  3     5.092195e+01  \n",
       "  4     5.092195e+01  \n",
       "  ...            ...  \n",
       "  2779  1.000000e-10  \n",
       "  2780  1.000000e-10  \n",
       "  2781  1.000000e-10  \n",
       "  2782  1.000000e-10  \n",
       "  2783  1.000000e-10  \n",
       "  \n",
       "  [2784 rows x 22 columns],\n",
       "           id       age  serBilir   serChol   albumin  alkaline      SGOT  \\\n",
       "  0       1.0  0.622822  0.350490  0.124321  0.209064  0.115642  0.109943   \n",
       "  1       2.0  0.578364  0.022059  0.149065  0.434211  0.529056  0.089506   \n",
       "  2       2.0  0.578364  0.022059  0.149065  0.434211  0.529056  0.089506   \n",
       "  3       2.0  0.578364  0.022059  0.149065  0.434211  0.529056  0.089506   \n",
       "  4       2.0  0.578364  0.022059  0.149065  0.434211  0.529056  0.089506   \n",
       "  ...     ...       ...       ...       ...       ...       ...       ...   \n",
       "  8352  134.0  0.307842  0.125000  0.252867  0.244152  0.178415  0.128295   \n",
       "  8353  135.0  0.319966  0.007353  0.121907  0.267544  0.031387  0.026527   \n",
       "  8354  137.0  0.701344  0.009804  0.173808  0.269006  0.055855  0.050717   \n",
       "  8355  140.0  0.532910  0.017157  0.132167  0.349415  0.049665  0.058225   \n",
       "  8356  141.0  0.397963  0.012255  0.136391  0.340643  0.046534  0.086670   \n",
       "  \n",
       "        platelets  prothrombin  histologic  ...  sex_male  ascites_Yes  \\\n",
       "  0      0.156842     0.266667    1.000000  ...       0.0          1.0   \n",
       "  1      0.189474     0.133333    0.666667  ...       0.0          0.0   \n",
       "  2      0.189474     0.133333    0.666667  ...       0.0          0.0   \n",
       "  3      0.189474     0.133333    0.666667  ...       0.0          0.0   \n",
       "  4      0.189474     0.133333    0.666667  ...       0.0          0.0   \n",
       "  ...         ...          ...         ...  ...       ...          ...   \n",
       "  8352   0.136842     0.316667    1.000000  ...       0.0          0.0   \n",
       "  8353   0.386316     0.183333    0.333333  ...       0.0          0.0   \n",
       "  8354   0.285263     0.200000    0.666667  ...       0.0          0.0   \n",
       "  8355   0.200000     0.216667    1.000000  ...       0.0          0.0   \n",
       "  8356   0.273684     0.133333    0.333333  ...       0.0          0.0   \n",
       "  \n",
       "        hepatomegaly_Yes  spiders_Yes  edema_edema despite diuretics  \\\n",
       "  0                  1.0          1.0                            1.0   \n",
       "  1                  1.0          1.0                            0.0   \n",
       "  2                  1.0          1.0                            0.0   \n",
       "  3                  1.0          1.0                            0.0   \n",
       "  4                  1.0          1.0                            0.0   \n",
       "  ...                ...          ...                            ...   \n",
       "  8352               1.0          0.0                            0.0   \n",
       "  8353               0.0          0.0                            0.0   \n",
       "  8354               1.0          0.0                            0.0   \n",
       "  8355               1.0          0.0                            0.0   \n",
       "  8356               0.0          0.0                            0.0   \n",
       "  \n",
       "        edema_edema no diuretics    LM      diff  bin        weight  \n",
       "  0                          0.0   0.0  0.000000  1.0  4.852301e-15  \n",
       "  1                          0.0   0.0  0.000000  1.0  6.539480e-41  \n",
       "  2                          0.0   0.0  0.000000  2.0  1.148236e-35  \n",
       "  3                          0.0   0.0  0.000000  3.0  6.591107e-31  \n",
       "  4                          0.0   0.0  0.000000  4.0  2.356758e-26  \n",
       "  ...                        ...   ...       ...  ...           ...  \n",
       "  8352                       0.0  10.0  0.113350  1.0  3.175296e-16  \n",
       "  8353                       0.0  10.0  1.356368  1.0  3.899237e-34  \n",
       "  8354                       0.0  10.0  0.515825  1.0  4.128405e-24  \n",
       "  8355                       0.0  10.0  0.165371  1.0  1.790719e-27  \n",
       "  8356                       0.0  10.0  8.973278  1.0  1.439645e-24  \n",
       "  \n",
       "  [8357 rows x 22 columns],\n",
       "            id       age  serBilir   serChol   albumin  alkaline      SGOT  \\\n",
       "  0        1.0  0.622822  0.350490  0.124321  0.209064  0.115642  0.109943   \n",
       "  1        1.0  0.622822  0.350490  0.124321  0.209064  0.115642  0.109943   \n",
       "  2        1.0  0.622822  0.350490  0.124321  0.209064  0.115642  0.109943   \n",
       "  3        1.0  0.622822  0.350490  0.124321  0.209064  0.115642  0.109943   \n",
       "  4        2.0  0.578364  0.022059  0.149065  0.434211  0.529056  0.089506   \n",
       "  ...      ...       ...       ...       ...       ...       ...       ...   \n",
       "  11131  140.0  0.532910  0.017157  0.132167  0.349415  0.049665  0.058225   \n",
       "  11132  141.0  0.397963  0.012255  0.136391  0.340643  0.046534  0.086670   \n",
       "  11133  141.0  0.397963  0.012255  0.136391  0.340643  0.046534  0.086670   \n",
       "  11134  141.0  0.397963  0.012255  0.136391  0.340643  0.046534  0.086670   \n",
       "  11135  141.0  0.397963  0.012255  0.136391  0.340643  0.046534  0.086670   \n",
       "  \n",
       "         platelets  prothrombin  histologic  ...  sex_male  ascites_Yes  \\\n",
       "  0       0.156842     0.266667    1.000000  ...       0.0          1.0   \n",
       "  1       0.156842     0.266667    1.000000  ...       0.0          1.0   \n",
       "  2       0.156842     0.266667    1.000000  ...       0.0          1.0   \n",
       "  3       0.156842     0.266667    1.000000  ...       0.0          1.0   \n",
       "  4       0.189474     0.133333    0.666667  ...       0.0          0.0   \n",
       "  ...          ...          ...         ...  ...       ...          ...   \n",
       "  11131   0.200000     0.216667    1.000000  ...       0.0          0.0   \n",
       "  11132   0.273684     0.133333    0.333333  ...       0.0          0.0   \n",
       "  11133   0.273684     0.133333    0.333333  ...       0.0          0.0   \n",
       "  11134   0.273684     0.133333    0.333333  ...       0.0          0.0   \n",
       "  11135   0.273684     0.133333    0.333333  ...       0.0          0.0   \n",
       "  \n",
       "         hepatomegaly_Yes  spiders_Yes  edema_edema despite diuretics  \\\n",
       "  0                   1.0          1.0                            1.0   \n",
       "  1                   1.0          1.0                            1.0   \n",
       "  2                   1.0          1.0                            1.0   \n",
       "  3                   1.0          1.0                            1.0   \n",
       "  4                   1.0          1.0                            0.0   \n",
       "  ...                 ...          ...                            ...   \n",
       "  11131               1.0          0.0                            0.0   \n",
       "  11132               0.0          0.0                            0.0   \n",
       "  11133               0.0          0.0                            0.0   \n",
       "  11134               0.0          0.0                            0.0   \n",
       "  11135               0.0          0.0                            0.0   \n",
       "  \n",
       "         edema_edema no diuretics    LM      diff  bin        weight  \n",
       "  0                           0.0   0.0  0.000000  1.0  4.587156e+01  \n",
       "  1                           0.0   0.0  0.000000  2.0  4.587156e+01  \n",
       "  2                           0.0   0.0  0.000000  3.0  4.587156e+01  \n",
       "  3                           0.0   0.0  0.000000  4.0  4.587156e+01  \n",
       "  4                           0.0   0.0  0.000000  1.0  5.092195e+01  \n",
       "  ...                         ...   ...       ...  ...           ...  \n",
       "  11131                       0.0  10.0  0.165371  4.0  1.000000e-10  \n",
       "  11132                       0.0  10.0  8.973278  1.0  1.000000e-10  \n",
       "  11133                       0.0  10.0  8.973278  2.0  1.000000e-10  \n",
       "  11134                       0.0  10.0  8.973278  3.0  1.000000e-10  \n",
       "  11135                       0.0  10.0  8.973278  4.0  1.000000e-10  \n",
       "  \n",
       "  [11136 rows x 22 columns]]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_noboot.inbags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37c6463d-c4ba-42de-a038-38a4c265829f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Empty DataFrame\n",
       "  Columns: [id, years, age, year, serBilir, serChol, albumin, alkaline, SGOT, platelets, prothrombin, histologic, status2, drug_placebo, sex_male, ascites_Yes, hepatomegaly_Yes, spiders_Yes, edema_edema despite diuretics, edema_edema no diuretics, weight_boot]\n",
       "  Index: []\n",
       "  \n",
       "  [0 rows x 21 columns],\n",
       "  Empty DataFrame\n",
       "  Columns: [id, years, age, serBilir, serChol, albumin, alkaline, SGOT, platelets, prothrombin, histologic, status2, drug_placebo, sex_male, ascites_Yes, hepatomegaly_Yes, spiders_Yes, edema_edema despite diuretics, edema_edema no diuretics, LM, diff, weight_boot]\n",
       "  Index: []\n",
       "  \n",
       "  [0 rows x 22 columns],\n",
       "  Empty DataFrame\n",
       "  Columns: [id, age, serBilir, serChol, albumin, alkaline, SGOT, platelets, prothrombin, histologic, status2, drug_placebo, sex_male, ascites_Yes, hepatomegaly_Yes, spiders_Yes, edema_edema despite diuretics, edema_edema no diuretics, LM, diff, bin, weight_boot]\n",
       "  Index: []\n",
       "  \n",
       "  [0 rows x 22 columns],\n",
       "  Empty DataFrame\n",
       "  Columns: [id, age, serBilir, serChol, albumin, alkaline, SGOT, platelets, prothrombin, histologic, status2, drug_placebo, sex_male, ascites_Yes, hepatomegaly_Yes, spiders_Yes, edema_edema despite diuretics, edema_edema no diuretics, LM, diff, bin, weight_boot]\n",
       "  Index: []\n",
       "  \n",
       "  [0 rows x 22 columns]]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_noboot.outbags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7f9bfeb-f2aa-49ca-aec5-37ae3e7d2475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.        , 0.82363086, 0.82363086, ..., 0.0625    , 0.0625    ,\n",
       "         0.0625    ],\n",
       "        [1.        , 0.90668355, 0.90668355, ..., 0.0625    , 0.0625    ,\n",
       "         0.0625    ],\n",
       "        [1.        , 0.74754589, 0.74754589, ..., 0.0625    , 0.0625    ,\n",
       "         0.0625    ],\n",
       "        ...,\n",
       "        [1.        , 0.96022085, 0.96022085, ..., 0.06251632, 0.0625    ,\n",
       "         0.0625    ],\n",
       "        [1.        , 0.88268026, 0.88268026, ..., 0.06249546, 0.0625    ,\n",
       "         0.0625    ],\n",
       "        [1.        , 0.95589151, 0.95589151, ..., 0.06249546, 0.0625    ,\n",
       "         0.0625    ]])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_noboot.train_supersets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab6d5453-aba6-4a9b-9549-370b7bb75b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00000000e+00, 2.50640413e-01, 2.50640413e-01, ...,\n",
       "         6.25102486e-02, 6.25000000e-02, 6.25000000e-02],\n",
       "        [1.00000000e+00, 2.42376242e-01, 2.42376242e-01, ...,\n",
       "         6.25102486e-02, 6.25000000e-02, 6.25000000e-02],\n",
       "        [1.00000000e+00, 9.65787264e-01, 9.65787264e-01, ...,\n",
       "         6.25102486e-02, 6.25000000e-02, 6.25000000e-02],\n",
       "        ...,\n",
       "        [1.00000000e+00, 9.40687647e-01, 9.40687647e-01, ...,\n",
       "         6.24691482e-02, 6.25000000e-02, 6.25000000e-02],\n",
       "        [0.00000000e+00, 6.41365824e-04, 6.41365824e-04, ...,\n",
       "         6.24691482e-02, 6.25000000e-02, 6.25000000e-02],\n",
       "        [1.00000000e+00, 8.04930806e-01, 8.04930806e-01, ...,\n",
       "         6.24691482e-02, 6.25000000e-02, 6.25000000e-02]])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_noboot.test_superset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8661796d-0665-4903-8e4a-1d44e196f61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2784,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_noboot.weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f2016-6f5f-4ac0-849a-1054ce1ca2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # store \n",
    "        self.inbags = IN_BAG_SETS\n",
    "        self.outbags = OUT_BAG_SETS\n",
    "        \n",
    "        self.train_supersets = BOOTSTRAP_SUPERSETS\n",
    "        self.test_superset = TEST_SUPER_SET\n",
    "        \n",
    "        self.weights = WEIGHT_BAG_SETS \n",
    "\n",
    "        # df1_k_train, df1_k_validation, df2_k_train, df2_k_validation, df3_k_train, df3_k_validation\n",
    "        return BOOTSTRAP_SUPERSETS, IN_BAG_SETS, OUT_BAG_SETS, WEIGHT_BAG_SETS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cd7cd-ef1e-44a5-8eb6-e1f8d90681aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdee6039-680d-4658-bb04-be323e64bf8a",
   "metadata": {},
   "source": [
    "# Working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c52a5-7491-4de5-91db-fcda7e386e95",
   "metadata": {},
   "source": [
    "## level 1 결과에 ID, LM, censoring ind, Time of event 달기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212c84d-6abd-46bb-a548-01382881d165",
   "metadata": {},
   "source": [
    "### train -> inbag set 중 lm1 버젼에서 lm, ID 로 정렬 후 superset_train에 붙이기\n",
    "### test -> test set lm1 버젼에서 lm, ID로 정렬 후 superset_test에 붙이기\n",
    "\n",
    "### 추가로 붓스트랩 횟수 30번으로 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c81ec1-6960-4fca-b4da-79e836abc7a5",
   "metadata": {},
   "source": [
    "## Metric 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02485bf7-352f-4096-8c3d-8bcf690b13a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffdf57d-576e-41cc-9f39-926fce2296dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa556bc-d684-4786-bf91-05e7a162ee92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b4314-d5c5-4ebc-9fb6-6375f546eda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa4523-b39a-4fa4-a6f2-b3557ec887ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c4ebf-b289-46e1-b3a3-283db017979c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5e656-e034-4e53-a335-d1893ecc96c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11cb44-bc98-4baf-a607-e7780d697844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7c006-de3f-4ec9-ab9f-3da89f7edf40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21165bf-45c2-41dc-910a-99ab4dd318f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
