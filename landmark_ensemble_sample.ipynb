{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be936cf-1ed8-4176-9510-5950348b239c",
   "metadata": {},
   "source": [
    "# CODE FOR LANDMARK ENSEMBLE\n",
    "## Without bagging ver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481ffe3-95a7-410e-8840-d4632d146afa",
   "metadata": {},
   "source": [
    "# 1. Load Data & simple Pre-processing & Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae49617-a74a-4234-80ab-c3225d9ea665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmark_ensemble.functions import * \n",
    "import dill\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45b6764-2e10-4e58-ac75-884e2ce510cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "\n",
    "# settings \n",
    "dir = \"/Users/pio/Google 드라이브/data/\"\n",
    "file_name = \"pbc2.csv\"\n",
    "data = pd.read_csv(dir + file_name)\n",
    "\n",
    "# drop status1 - competing risks setting\n",
    "data = data.drop(axis=1, columns =['status'])\n",
    "\n",
    "\n",
    "# ID, Time, Event, Measure Time column names\n",
    "ID_col = 'id'; T_col ='years'; E_col ='status2'; measure_T_col = 'year'\n",
    "\n",
    "# categorical variables\n",
    "nominal_col = ['drug','sex', 'ascites', 'hepatomegaly','spiders', 'edema']\n",
    "ordinal_col = ['histologic']\n",
    "\n",
    "# continuous variables\n",
    "cont_col = list(set(data.columns) - set(nominal_col) - set(ordinal_col) - set([ID_col, T_col, E_col, measure_T_col]))\n",
    "\n",
    "# window - 5 year prediction \n",
    "window = 5\n",
    "\n",
    "# S : landmark time points - 0, 0.5, 1, ..., 10\n",
    "S = np.linspace(0,10,21)\n",
    "v_years = S+window\n",
    "\n",
    "# Number of bins when discritizing \n",
    "## !!!(Actually, k_bin - 1 bins are produced)!!!\n",
    "k_bin = 5\n",
    "\n",
    "# minimal bin_size\n",
    "minimal_bin_size = window / (k_bin-1)\n",
    "\n",
    "# \n",
    "\n",
    "# for continous variables, \n",
    "## scaling -> min-max scaling &\n",
    "## imputation -> fill na's : median for continous\n",
    "for col in cont_col : \n",
    "    data[col] = data[col].fillna(data[col].median())\n",
    "    data[col] = (data[col] - min(data[col])) / (max(data[col]) - min(data[col]))\n",
    "\n",
    "# one-hot encoding for categorical variables\n",
    "data = pd.get_dummies(data, columns = nominal_col, drop_first=True)\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "# settings2\n",
    "\n",
    "# proportion of train set\n",
    "p_train = 0.7\n",
    "k_kfold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a408e62-3d30-4016-bd1a-56a2d287beb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_instance</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cox_str</td>\n",
       "      <td>&lt;lifelines.CoxPHFitter&gt;</td>\n",
       "      <td>{'penalizer': [0.006737946999085467, 0.0301973...</td>\n",
       "      <td>cox_str</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cox_no_str</td>\n",
       "      <td>&lt;lifelines.CoxPHFitter&gt;</td>\n",
       "      <td>{'penalizer': [0.006737946999085467, 0.0301973...</td>\n",
       "      <td>cox_no_str</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'p...</td>\n",
       "      <td>lr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>{'n_estimators': [50, 100, 300, 500], 'max_dep...</td>\n",
       "      <td>rf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier()</td>\n",
       "      <td>{'n_estimators': [50, 100, 300, 500], 'max_dep...</td>\n",
       "      <td>gb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>{'hidden_layer_sizes': [1, 2], 'activation': [...</td>\n",
       "      <td>mlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>{'n_neighbors': [1, 5, 10], 'weights': ['unifo...</td>\n",
       "      <td>knn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NGB</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>{'var_smoothing': [1e-05, 1e-09, 0.1]}</td>\n",
       "      <td>ngb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ADA</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>{'n_estimators': [50, 100, 300, 500], 'max_dep...</td>\n",
       "      <td>ada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_name                      model_instance  \\\n",
       "0     cox_str             <lifelines.CoxPHFitter>   \n",
       "1  cox_no_str             <lifelines.CoxPHFitter>   \n",
       "2          LR  LogisticRegression(max_iter=10000)   \n",
       "3          RF            RandomForestClassifier()   \n",
       "4          GB        GradientBoostingClassifier()   \n",
       "5         MLP                     MLPClassifier()   \n",
       "6         KNN              KNeighborsClassifier()   \n",
       "7         NGB                        GaussianNB()   \n",
       "8         ADA                AdaBoostClassifier()   \n",
       "\n",
       "                                         hyperparams        type  \n",
       "0  {'penalizer': [0.006737946999085467, 0.0301973...     cox_str  \n",
       "1  {'penalizer': [0.006737946999085467, 0.0301973...  cox_no_str  \n",
       "2  {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'p...          lr  \n",
       "3  {'n_estimators': [50, 100, 300, 500], 'max_dep...          rf  \n",
       "4  {'n_estimators': [50, 100, 300, 500], 'max_dep...          gb  \n",
       "5  {'hidden_layer_sizes': [1, 2], 'activation': [...         mlp  \n",
       "6  {'n_neighbors': [1, 5, 10], 'weights': ['unifo...         knn  \n",
       "7             {'var_smoothing': [1e-05, 1e-09, 0.1]}         ngb  \n",
       "8  {'n_estimators': [50, 100, 300, 500], 'max_dep...         ada  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## model specifics of level 0 models\n",
    "cox_params = {'penalizer':np.exp(np.linspace(-5,1,5)),'l1_ratio':[0,0.25,0.5,0.75,1]}\n",
    "# 5*5 *2 = 50\n",
    "model_specifics_cont = pd.DataFrame({'model_name' : ['cox_str', 'cox_no_str'], \n",
    "                                'model_instance':[CoxPHFitter(),CoxPHFitter()], \n",
    "                                'hyperparams':[cox_params,cox_params], \n",
    "                                'type':['cox_str','cox_no_str']})\n",
    "\n",
    "LR_params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga']\n",
    "} # 7 * 2 * 1 = 14\n",
    "RF_params = {'n_estimators':[50,100,300,500],'max_depth':[1,3,5]} # 4*3 = 12\n",
    "GB_params = {'n_estimators':[50,100,300,500],'max_depth':[1,3,5]} # 4*3 = 12\n",
    "MLP_params = {'hidden_layer_sizes':[1,2], 'activation' : ['logistic', 'relu'], 'max_iter' : [1000], 'early_stopping' : [True], 'learning_rate' : ['adaptive']}\n",
    "# 2*2 = 4\n",
    "KNN_params = {'n_neighbors':[1,5,10], 'weights':['uniform', 'distance']} \n",
    "# 3*2\n",
    "NGB_params = {'var_smoothing':[1e-5, 1e-9, 1e-1]}\n",
    "# 3\n",
    "ADA_params = {'n_estimators':[50, 100, 300, 500], 'max_depth':[1,3,5]}\n",
    "# 4*10*3 = 36\n",
    "\n",
    "model_specifics_disc = pd.DataFrame({'model_name' : ['LR','RF','GB','MLP','KNN','NGB','ADA'], \n",
    "                                'model_instance':[LogisticRegression(max_iter=10000),RandomForestClassifier(),GradientBoostingClassifier(),MLPClassifier(),KNeighborsClassifier(),GaussianNB(), AdaBoostClassifier()], \n",
    "                                'hyperparams':[LR_params, RF_params, GB_params,MLP_params, KNN_params,NGB_params, ADA_params], \n",
    "                                'type':['lr','rf','gb','mlp','knn','ngb','ada']})\n",
    "\n",
    "\n",
    "model_specifics = pd.concat([model_specifics_cont,model_specifics_disc],axis=0).reset_index(drop=True)\n",
    "model_specifics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833b744-145d-463d-a8e1-d0f55a9c31b2",
   "metadata": {},
   "source": [
    "# 2. Landmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee64c7f2-f81e-4fbc-b65b-882d5e33aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm_cont = landmarker_cont(data=data, ID_col = ID_col, T_col = T_col, E_col = E_col, \n",
    "                window = window, S= S, measure_T_col = measure_T_col)\n",
    "\n",
    "data_lm_disc = landmarker_disc(data=data_lm_cont,ID_col = ID_col, T_col = T_col, E_col = E_col, \n",
    "                window = window, S= S, measure_T_col = measure_T_col, k_bin = k_bin, train=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d03d57-5bac-47e7-8000-682841b23b09",
   "metadata": {},
   "source": [
    "# 3. Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617b2a4d-5543-433d-93ed-5bfa1a9f0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save = '/Users/pio/Google 드라이브/github/survival ensemble/experiment/'\n",
    "n_rep = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b960e51-995f-495a-80db-72916fff2346",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Current Seed = 30\n",
      "Current Time = 01:06 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 31\n",
      "Current Time = 01:25 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 32\n",
      "Current Time = 01:42 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 33\n",
      "Current Time = 02:01 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 34\n",
      "Current Time = 02:20 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 35\n",
      "Current Time = 02:38 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 36\n",
      "Current Time = 02:56 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 37\n",
      "Current Time = 03:15 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 38\n",
      "Current Time = 03:33 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 39\n",
      "Current Time = 03:52 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 40\n",
      "Current Time = 04:10 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 41\n",
      "Current Time = 04:29 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 42\n",
      "Current Time = 04:48 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 43\n",
      "Current Time = 05:07 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 44\n",
      "Current Time = 05:25 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 45\n",
      "Current Time = 05:44 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 46\n",
      "Current Time = 06:03 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 47\n",
      "Current Time = 06:22 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 48\n",
      "Current Time = 06:41 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n",
      "##############################\n",
      "Current Seed = 49\n",
      "Current Time = 06:59 \n",
      "Generating dataset for training meta model\n",
      "fold : 0\n",
      "fold : 1\n",
      "fold : 2\n",
      "Re-train baseline models\n"
     ]
    }
   ],
   "source": [
    "for seed_num in range(n_rep) : \n",
    "    current_time = datetime.now().strftime('%H:%M ')\n",
    "    print('##############################')\n",
    "    print('Current Seed =', seed_num)\n",
    "    print('Current Time =', current_time)\n",
    "\n",
    "    ####### <TRAIN-TEST SPLIT>\n",
    "    # Split IDs into train set and test set\n",
    "    train_id, test_id = id_train_test_split(id_list = data[ID_col], seed_number = seed_num, p=0.7)\n",
    "\n",
    "    # Train, test set from original form\n",
    "    train = data[data[ID_col].isin(train_id)].reset_index(drop=True)\n",
    "    test = data[data[ID_col].isin(test_id)].reset_index(drop=True)\n",
    "\n",
    "    # Train, test set for continous landmarking algorithms\n",
    "    train_lm_cont = data_lm_cont[data_lm_cont[ID_col].isin(train_id)].reset_index(drop=True)\n",
    "    test_lm_cont = data_lm_cont[data_lm_cont[ID_col].isin(test_id)].reset_index(drop=True)\n",
    "\n",
    "    # Train, test set for discrete landmarking algorithms\n",
    "    train_lm_disc = data_lm_disc[data_lm_disc[ID_col].isin(train_id)].reset_index(drop=True)\n",
    "    test_lm_disc = data_lm_disc[data_lm_disc[ID_col].isin(test_id)].reset_index(drop=True)\n",
    "\n",
    "#    print(np.all(np.unique(train_lm_cont.id) == np.unique(train_lm_disc.id)))\n",
    "#    print(np.all(np.unique(test_lm_cont.id) == np.unique(test_lm_disc.id)))\n",
    "\n",
    "    ####### <TRAINING PART>\n",
    "    # 1. Generating dataset for training meta model part \n",
    "    \n",
    "    kfold = id_kfold(id_list=train_id, n_split=k_kfold,seed_number=seed_num)\n",
    "    stacked_trn = []\n",
    "    print('Generating dataset for training meta model')\n",
    "    for i in range(k_kfold) : \n",
    "        print('fold : ' + str(i))\n",
    "        k_fold_trn_id, k_fold_val_id = next(kfold)\n",
    "\n",
    "        k_fold_trn_lm_cont = train_lm_cont[train_lm_cont[ID_col].isin(k_fold_trn_id)].copy()\n",
    "        k_fold_trn_lm_disc = train_lm_disc[train_lm_disc[ID_col].isin(k_fold_trn_id)].copy()\n",
    "\n",
    "        k_fold_val_lm_cont = train_lm_cont[train_lm_cont[ID_col].isin(k_fold_val_id)].copy()\n",
    "        k_fold_val_lm_disc = train_lm_disc[train_lm_disc[ID_col].isin(k_fold_val_id)].copy()\n",
    "\n",
    "        # fit all baseline models        \n",
    "        stack_fit = stacker(model_specifics = model_specifics, \n",
    "                            ID = ID_col, T = T_col, E = E_col, S = S, window = window, k_bin = k_bin)\n",
    "        stack_fit.fit(data_cont= k_fold_trn_lm_cont , data_disc = k_fold_trn_lm_disc) \n",
    "\n",
    "        # stack them for training meta model\n",
    "        stacked_trn.append(stack_fit.predict(k_fold_val_lm_cont, k_fold_val_lm_disc))\n",
    "\n",
    "    # ID_col, LM, T_col, E_col validation 순서에 맞게 모으기\n",
    "    info = pd.concat([train_lm_cont[train_lm_cont[ID_col].isin(kfold.validation_fold_id[i])][[ID_col, 'LM', T_col, E_col]].reset_index(drop=True) for i in range(len(stacked_trn))], ignore_index=True)\n",
    "    # kfold validation 예측 결과 모으기\n",
    "    pred = b = pd.concat([pd.DataFrame(stacked_trn[i]) for i in range(len(stacked_trn))], ignore_index=True)\n",
    "    new_data = pd.concat([info,pred], axis=1)\n",
    "    # new_data['surv_status'] = abs(new_data[E_col]-1)\n",
    "    ######\n",
    "    # 2. Training Part : \n",
    "    # 2-1. (Re-)train baseline models on whole dataset  \n",
    "    print('Re-train baseline models')\n",
    "    stack_fit = stacker(model_specifics = model_specifics, \n",
    "                            ID = ID_col, T = T_col, E = E_col, S = S, window = window, k_bin = k_bin)\n",
    "\n",
    "    stack_fit.fit(data_cont=train_lm_cont.copy() , data_disc = train_lm_disc.copy()) \n",
    "\n",
    "    ##### \n",
    "    # 2-2-1. calculating ipcw weights( for supplying weights to meta models)\n",
    "    ipcw_calc = ipcw_fitter(S= S, window =window)\n",
    "    ipcw_calc.fit(data= new_data, T = T_col, E = E_col)\n",
    "\n",
    "    # 2-2-2. Meta model Training 및 3. Prediction으로 연결.  \n",
    "    \n",
    "    ####### <SAVE PART>\n",
    "    # \n",
    "    with open(dir_save +'seed_'+str(seed_num)+'_'+'new_data.pkl', 'wb') as f : \n",
    "        dill.dump(new_data,f)\n",
    "\n",
    "    with open(dir_save +'seed_'+str(seed_num)+'_'+'stack_fit.pkl', 'wb') as f : \n",
    "        dill.dump(stack_fit,f)\n",
    "\n",
    "    with open(dir_save +'seed_'+str(seed_num)+'_'+'ipcw_fit.pkl', 'wb') as f : \n",
    "        dill.dump(ipcw_calc,f)\n",
    "    \n",
    "    train_test_id_dict = {'train_id':train_id, 'test_id':test_id}\n",
    "    with open(dir_save +'seed_'+str(seed_num)+'_'+'train_test_id_dict.pkl', 'wb') as f : \n",
    "        dill.dump(train_test_id_dict,f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261ef54-f14a-4489-82a6-229b8fed2628",
   "metadata": {},
   "source": [
    "# 4. Load models  & data again and fit meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de96dc-5fd0-4c1a-b392-1905654fd678",
   "metadata": {},
   "source": [
    "### 메트릭 계산 안 되는 seed : 8, 12, 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "692b624d-3020-43d4-a56b-1b8ed2560a8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "28\n",
      "(2847, 113)\n",
      "(2847,)\n",
      "(2847,)\n",
      "0\n",
      "0\n",
      "########\n",
      "29\n",
      "(2729, 113)\n",
      "(2729,)\n",
      "(2729,)\n",
      "0\n",
      "0\n",
      "########\n",
      "30\n",
      "(2764, 113)\n",
      "(2764,)\n",
      "(2764,)\n",
      "0\n",
      "0\n",
      "########\n",
      "31\n",
      "(2798, 113)\n",
      "(2798,)\n",
      "(2798,)\n",
      "0\n",
      "0\n",
      "########\n",
      "32\n",
      "(2806, 113)\n",
      "(2806,)\n",
      "(2806,)\n",
      "0\n",
      "0\n",
      "########\n",
      "33\n",
      "(2836, 113)\n",
      "(2836,)\n",
      "(2836,)\n",
      "0\n",
      "0\n",
      "########\n",
      "34\n",
      "(2835, 113)\n",
      "(2835,)\n",
      "(2835,)\n",
      "0\n",
      "0\n",
      "########\n",
      "35\n",
      "(2855, 113)\n",
      "(2855,)\n",
      "(2855,)\n",
      "0\n",
      "0\n",
      "########\n",
      "36\n",
      "(2737, 113)\n",
      "(2737,)\n",
      "(2737,)\n",
      "0\n",
      "0\n",
      "########\n",
      "37\n",
      "(2747, 113)\n",
      "(2747,)\n",
      "(2747,)\n",
      "0\n",
      "0\n",
      "########\n",
      "38\n",
      "(2755, 113)\n",
      "(2755,)\n",
      "(2755,)\n",
      "0\n",
      "0\n",
      "########\n",
      "39\n",
      "(2681, 113)\n",
      "(2681,)\n",
      "(2681,)\n",
      "0\n",
      "0\n",
      "########\n",
      "40\n",
      "(2834, 113)\n",
      "(2834,)\n",
      "(2834,)\n",
      "0\n",
      "0\n",
      "########\n",
      "41\n",
      "(2853, 113)\n",
      "(2853,)\n",
      "(2853,)\n",
      "0\n",
      "0\n",
      "########\n",
      "42\n",
      "(2771, 113)\n",
      "(2771,)\n",
      "(2771,)\n",
      "0\n",
      "0\n",
      "########\n",
      "43\n",
      "(2820, 113)\n",
      "(2820,)\n",
      "(2820,)\n",
      "0\n",
      "0\n",
      "########\n",
      "44\n",
      "(2777, 113)\n",
      "(2777,)\n",
      "(2777,)\n",
      "0\n",
      "0\n",
      "########\n",
      "45\n",
      "(2836, 113)\n",
      "(2836,)\n",
      "(2836,)\n",
      "0\n",
      "0\n",
      "########\n",
      "46\n",
      "(2772, 113)\n",
      "(2772,)\n",
      "(2772,)\n",
      "0\n",
      "0\n",
      "########\n",
      "47\n",
      "(2763, 113)\n",
      "(2763,)\n",
      "(2763,)\n",
      "0\n",
      "0\n",
      "########\n",
      "48\n",
      "(2788, 113)\n",
      "(2788,)\n",
      "(2788,)\n",
      "0\n",
      "0\n",
      "########\n",
      "49\n",
      "(2834, 113)\n",
      "(2834,)\n",
      "(2834,)\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dir_save = '/Users/pio/Google 드라이브/github/survival ensemble/experiment/'\n",
    "\n",
    "for seed_num in range(28,50) : \n",
    "    print('########')\n",
    "    print(seed_num)\n",
    "    # loading\n",
    "    with open(dir_save +'seed_'+str(seed_num)+'_'+'new_data.pkl', 'rb') as f : \n",
    "        new_data = dill.load(f)\n",
    "\n",
    "    with open(dir_save +'seed_'+str(seed_num)+'_'+'stack_fit.pkl', 'rb') as f : \n",
    "        stack_fit = dill.load(f)\n",
    "\n",
    "    with open(dir_save +'seed_'+str(seed_num)+'_'+'ipcw_fit.pkl', 'rb') as f : \n",
    "        ipcw_calc = dill.load(f)\n",
    "\n",
    "    with open(dir_save +'seed_'+str(seed_num)+'_'+'train_test_id_dict.pkl', 'rb') as f : \n",
    "        train_test_id_dict = dill.load(f)\n",
    "\n",
    "\n",
    "    new_x = new_data.drop([ID_col, 'LM', T_col, E_col],axis=1)\n",
    "    new_y = abs(new_data[E_col]-1)  # 생존확률의 결합이므로 라벨을 뒤집어줘야 함.\n",
    "    new_w = ipcw_calc.predict(new_data) # \n",
    "\n",
    "    print(new_x.shape)\n",
    "    print(new_y.shape)\n",
    "    print(new_w.shape)\n",
    "\n",
    "    train_lm_cont = data_lm_cont[data_lm_cont[ID_col].isin(train_test_id_dict['train_id'])].reset_index(drop=True)\n",
    "    train_lm_disc = data_lm_disc[data_lm_disc[ID_col].isin(train_test_id_dict['train_id'])].reset_index(drop=True)\n",
    "\n",
    "    test_lm_cont = data_lm_cont[data_lm_cont[ID_col].isin(train_test_id_dict['test_id'])].reset_index(drop=True)\n",
    "    test_lm_disc = data_lm_disc[data_lm_disc[ID_col].isin(train_test_id_dict['test_id'])].reset_index(drop=True)\n",
    "\n",
    "    # fit meta model\n",
    "    nnls = nnls_constraint()\n",
    "    nnls.fit(x = new_x, \n",
    "             y = new_y,\n",
    "             w = new_w)\n",
    "\n",
    "\n",
    "    hill = hillclimb()\n",
    "    hill.fit(x = new_x, \n",
    "             y = new_y,\n",
    "             w = new_w)\n",
    "\n",
    "\n",
    "    ipcw_rf = RandomForestClassifier()\n",
    "    ipcw_rf.fit(X = new_x, \n",
    "                y = new_y, \n",
    "                sample_weight = new_w) \n",
    "\n",
    "    # predict\n",
    "    test_new_x = stack_fit.predict(data_cont = test_lm_cont, data_disc = test_lm_disc) \n",
    "\n",
    "    nnls_pred = nnls.predict(test_new_x)\n",
    "    hill_pred = hill.predict(test_new_x)\n",
    "    ipcw_rf_pred = ipcw_rf.predict_proba(test_new_x)[:,1]\n",
    "    \n",
    "    # scores\n",
    "    ## Brier\n",
    "    \n",
    "    test_ipcw_calc = ipcw_fitter(S= S, window =window)\n",
    "    test_ipcw_calc.fit(data= test_lm_cont, T = T_col, E = E_col)\n",
    "    test_ipcw_pred = test_ipcw_calc.predict(data= test_lm_cont)\n",
    "\n",
    "    # i for model, j for landmarked time\n",
    "    brier_score_list = []\n",
    "    for i in range(test_new_x.shape[1]) : \n",
    "        temp = []\n",
    "        for j in S : \n",
    "            value = brier_score_loss(y_true = abs(test_lm_cont[E_col]-1)[test_lm_cont['LM'] == j], \n",
    "                             y_prob = pd.DataFrame(test_new_x)[test_lm_cont['LM'] == j][i], \n",
    "                             sample_weight= test_ipcw_pred[test_lm_cont['LM'] == j])\n",
    "            temp.append(value)        \n",
    "        brier_score_list.append(temp)\n",
    "\n",
    "    result_brier_base = pd.DataFrame(brier_score_list)\n",
    "    \n",
    "    brier_nnls = [] ; brier_hill = [] ; brier_rf = []\n",
    "    for j in S : \n",
    "    #    brier_cox.append(brier_score_loss(y_true = abs(test_lm_cont[E_col]-1)[test_lm_cont['LM'] == j], \n",
    "    #                         y_prob = pd.DataFrame(test_new_x)[test_lm_cont['LM'] == j][75], \n",
    "    #                         sample_weight= test_ipcw_pred[test_lm_cont['LM'] == j]))\n",
    "        brier_nnls.append(brier_score_loss(y_true = abs(test_lm_cont[E_col]-1)[test_lm_cont['LM'] == j], \n",
    "                         y_prob = nnls_pred[test_lm_cont['LM'] == j], \n",
    "                         sample_weight= test_ipcw_pred[test_lm_cont['LM'] == j]))        \n",
    "        brier_hill.append(brier_score_loss(y_true = abs(test_lm_cont[E_col]-1)[test_lm_cont['LM'] == j], \n",
    "                         y_prob = hill_pred[test_lm_cont['LM'] == j], \n",
    "                         sample_weight= test_ipcw_pred[test_lm_cont['LM'] == j]))\n",
    "        brier_rf.append(brier_score_loss(y_true = abs(test_lm_cont[E_col]-1)[test_lm_cont['LM'] == j], \n",
    "                         y_prob = ipcw_rf_pred[test_lm_cont['LM'] == j], \n",
    "                         sample_weight= test_ipcw_pred[test_lm_cont['LM'] == j]))\n",
    "\n",
    "\n",
    "    result_brier_meta = pd.DataFrame({'nnls': brier_nnls, 'hill':brier_hill, 'rf':brier_rf})\n",
    "    result_brier_total = pd.concat([pd.DataFrame(np.array(result_brier_base).T), result_brier_meta],axis=1)\n",
    "         \n",
    "    with open(dir_save +'seed_'+str(seed_num)+'_'+'brier_score.pkl', 'wb') as f : \n",
    "        dill.dump(result_brier_total,f)\n",
    "        \n",
    "    ## C-index\n",
    "    # i for model, j for landmarked time\n",
    "    c_index_list = []\n",
    "    for i in range(test_new_x.shape[1]) : \n",
    "        temp = []\n",
    "        for j in S : \n",
    "            c_index_value = concordance_index(event_times = test_lm_cont[test_lm_cont['LM'] == j][T_col], \n",
    "                                              predicted_scores = pd.DataFrame(test_new_x)[test_lm_cont['LM'] == j][i],\n",
    "                                              event_observed = test_lm_cont[test_lm_cont['LM'] == j][E_col])\n",
    "            temp.append(c_index_value)        \n",
    "        c_index_list.append(temp)\n",
    "\n",
    "    result_c_base = pd.DataFrame(c_index_list)\n",
    "    \n",
    "    \n",
    "    c_index_nnls = []; c_index_hill = []; c_index_rf = [] \n",
    "    for j in S : \n",
    "        c_index_nnls.append(concordance_index(event_times = test_lm_cont[test_lm_cont['LM'] == j][T_col], \n",
    "                      predicted_scores = nnls_pred[test_lm_cont['LM'] == j],\n",
    "                      event_observed = test_lm_cont[test_lm_cont['LM'] == j][E_col])\n",
    "\n",
    "    )        \n",
    "        c_index_hill.append(concordance_index(event_times = test_lm_cont[test_lm_cont['LM'] == j][T_col], \n",
    "                      predicted_scores = hill_pred[test_lm_cont['LM'] == j],\n",
    "                      event_observed = test_lm_cont[test_lm_cont['LM'] == j][E_col])\n",
    "\n",
    "    )\n",
    "        c_index_rf.append(concordance_index(event_times = test_lm_cont[test_lm_cont['LM'] == j][T_col], \n",
    "                      predicted_scores = ipcw_rf_pred[test_lm_cont['LM'] == j],\n",
    "                      event_observed = test_lm_cont[test_lm_cont['LM'] == j][E_col])\n",
    "\n",
    "    )\n",
    "\n",
    "    result_c_meta = pd.DataFrame({'nnls': c_index_nnls, 'hill':c_index_hill, 'rf':c_index_rf})\n",
    "    result_c_total = pd.concat([pd.DataFrame(np.array(result_c_base).T),result_c_meta], axis=1)\n",
    "    with open(dir_save +'seed_'+str(seed_num)+'_'+'c_index_score.pkl', 'wb') as f : \n",
    "        dill.dump(result_c_total,f)\n",
    "\n",
    "        \n",
    "    print(sum(np.sum(np.isnan(result_brier_total))))\n",
    "    print(sum(np.sum(np.isnan(result_c_total))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
