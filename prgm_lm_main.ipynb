{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bce891c-c874-4579-bdf2-e464836949d2",
   "metadata": {},
   "source": [
    "# Program for Landmarking\n",
    "\n",
    "# Data loading and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae56c52e-d022-4aac-8898-6ac74e8ada93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_ipcw, concordance_index_censored\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# models \n",
    "from lifelines import CoxPHFitter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# others\n",
    "from numpy import inf\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6fe868c-6fd0-418a-935d-edc34cf900db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENS SURV module\n",
    "from ens_surv.utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f097b753-4339-4048-98e4-d9f117b65cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "# loading data & preprop\n",
    "\n",
    "# settings \n",
    "dir = \"/Users/pio/Google 드라이브/papers_related/graduation thesis/programs/\"\n",
    "file_name = \"pbc.csv\"\n",
    "data = pd.read_csv(dir + file_name)\n",
    "\n",
    "# drop status1 - competing risks setting\n",
    "data = data.drop(axis=1, columns =['status'])\n",
    "\n",
    "\n",
    "# ID, Time, Event, Measure Time column names\n",
    "ID_col = 'id'; T_col ='years'; E_col ='status2'; measure_T_col = 'year'\n",
    "\n",
    "# categorical variables\n",
    "nominal_col = ['drug','sex', 'ascites', 'hepatomegaly','spiders', 'edema']\n",
    "ordinal_col = ['histologic']\n",
    "\n",
    "# continuous variables\n",
    "cont_col = list(set(data.columns) - set(nominal_col) - set(ordinal_col) - set([ID_col, T_col, E_col, measure_T_col]))\n",
    "\n",
    "# window - 5 year prediction \n",
    "window = 5\n",
    "\n",
    "# S : landmark time points - 0, 0.5, 1, ..., 10\n",
    "S = np.linspace(0,10,21)\n",
    "v_years = S+window\n",
    "\n",
    "# Number of bins when discritizing \n",
    "## !!!(Actually, k_bin - 1 bins are produced)!!!\n",
    "k_bin = 5\n",
    "\n",
    "# minimal bin_size\n",
    "minimal_bin_size = window / (k_bin-1)\n",
    "# t_grid -> minimal points where survival probabilities are measured\n",
    "# t_grid = np.arange(0,S[-1] + window + minimal_bin_size, step = minimal_bin_size)\n",
    "\n",
    "# imputation -> fill na's : median for continous\n",
    "for col in cont_col : \n",
    "    data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "\n",
    "# one-hot encoding for categorical variables\n",
    "data = pd.get_dummies(data, columns = nominal_col, drop_first=True)\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "# settings2\n",
    "\n",
    "# proportion of train set\n",
    "p_train = 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858a366-1175-4fe7-9c5b-425720ed8dbd",
   "metadata": {},
   "source": [
    "# 빨리 빨리 디버깅하려고 data 자름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0ded9a-ffd8-4904-8ba9-0653cd0e2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.head(100)\n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534d4d9-138e-4779-b903-3b14a0061851",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eece761-218c-45bf-a40c-13251bd8f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n",
      "(36, 20)\n",
      "Intersection :  set()\n"
     ]
    }
   ],
   "source": [
    "train, test = splitID(data = data,ID_col = ID_col, p = p_train)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print('Intersection : ', set(np.unique(train[ID_col])).intersection(set(np.unique(test[ID_col]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a944252-c42d-4776-9e77-e2046508f5f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lm1 = LM_transformer(df=train,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col)\n",
    "test_lm1 = LM_transformer(df=test,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col)\n",
    "\n",
    "train_lm2_train_ver = LM_transformer2(df=train_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=True)\n",
    "train_lm2_validation_ver = LM_transformer2(df=train_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=False)\n",
    "\n",
    "test_lm2 = LM_transformer2(df=test_lm1,ID_col = ID_col,T_col=T_col,E_col=E_col,window=window,S=S,measure_T_col=measure_T_col,k_bin = k_bin, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92048e4-cae3-48e9-9078-b1072817953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n",
      "(36, 20)\n",
      "(111, 21)\n",
      "(67, 21)\n",
      "(319, 21)\n",
      "(444, 21)\n",
      "(268, 21)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print(train_lm1.shape)\n",
    "print(test_lm1.shape)\n",
    "\n",
    "print(train_lm2_train_ver.shape)\n",
    "print(train_lm2_validation_ver.shape)\n",
    "\n",
    "print(test_lm2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d7d3d-6856-4c85-ba55-a04d6136b061",
   "metadata": {},
   "source": [
    "# BOOT & K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "098b7dce-8d14-4dcf-9382-1b0bda296576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting : \n",
    "\n",
    "# B : number of resampling / K : number of folds / boot : replacement true false\n",
    "B = 1\n",
    "K = 3\n",
    "boot = False\n",
    "\n",
    "\n",
    "base_info = {'ID_col':ID_col, 'T_col':T_col, 'E_col':E_col, 'measure_T_col':measure_T_col, 'boot':boot, 'B':B, 'K':K, \n",
    "            'window':window , 'S' :S, 'k_bin':k_bin}\n",
    "\n",
    "# df list : in order of original, landmark 1, landmark 2(disc) train version, landmark 2(disc) validation ver \n",
    "train_df_list = [train, train_lm1, train_lm2_train_ver, train_lm2_validation_ver]\n",
    "test_df_list = [test, test_lm1, test_lm2]\n",
    "\n",
    "# model specifics : model name & model instance & hyperparameter grid & type of model\n",
    "## type of model : cont(continous) or disc(discrete)\n",
    "\n",
    "## model specifics of level 1 models\n",
    "cox1_params = {'penalizer':[0,0.5],'l1_ratio':[0,1]}\n",
    "\n",
    "model_specifics_cont = pd.DataFrame({'model_name' : ['cox1'], \n",
    "                                'model_instance':[CoxPHFitter()], \n",
    "                                'hyperparams':[cox1_params], \n",
    "                                'type':['cont']})\n",
    "\n",
    "LR_params = {'C':[0.05,  10]}\n",
    "RF_params = {'n_estimators':[10,50,100],'max_depth':[1,5]}\n",
    "GB_params = {'n_estimators':[10,50,100],'max_depth':[1,5]}\n",
    "\n",
    "model_specifics_disc = pd.DataFrame({'model_name' : ['LR','RF','GB'], \n",
    "                                'model_instance':[LogisticRegression(max_iter=10000),RandomForestClassifier(),GradientBoostingClassifier()], \n",
    "                                'hyperparams':[LR_params, RF_params, GB_params], \n",
    "                                'type':['disc','disc','disc']})\n",
    "\n",
    "\n",
    "model_specifics_1 = pd.concat([model_specifics_cont,model_specifics_disc],axis=0).reset_index(drop=True)\n",
    "\n",
    "## model specifics of level 2 models\n",
    "model_specifics_2 = pd.DataFrame({'model_name':['M1'], \n",
    "                                  'model_instance':[LogisticRegression(max_iter=10000)],\n",
    "                                  'hyperparams':[{'C':[0.05, 10]}],\n",
    "                                 })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bdafc7-d46c-4e58-9b3d-46aa04de6da9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761cf8e-7707-453d-99fc-5f510d32d2a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5bdfca5-aa85-4f3b-8c49-2f5a1f6e25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ens_surv.boot_kfold import boot_kfold\n",
    "\n",
    "\n",
    "bk1 = boot_kfold(base_info = base_info, \n",
    "           train_df_list = train_df_list, \n",
    "           test_df_list = test_df_list,\n",
    "           model_specifics_1 = model_specifics_1, \n",
    "           model_specifics_2 = model_specifics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd7a522-6191-4070-b7e1-2dca8bec2c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "1 / 1  Resampled\n",
      "1 / 3  fold\n",
      "$$$\n",
      "Iteration :  1\n",
      "cox1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pio/opt/anaconda3/envs/dyndeephit/lib/python3.6/site-packages/lifelines/utils/__init__.py:1115: ConvergenceWarning: Column ascites_Yes have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['status2'].astype(bool)\n",
      ">>> print(df.loc[events, 'ascites_Yes'].var())\n",
      ">>> print(df.loc[~events, 'ascites_Yes'].var())\n",
      "\n",
      "A very low variance means that the column ascites_Yes completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/Users/pio/opt/anaconda3/envs/dyndeephit/lib/python3.6/site-packages/lifelines/fitters/coxph_fitter.py:701: ConvergenceWarning: Newton-Rhaphson failed to converge sufficiently in 500 steps.\n",
      "\n",
      "  warnings.warn(\"Newton-Rhaphson failed to converge sufficiently in %d steps.\\n\" % max_steps, ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'v_years' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7346e71df6bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbk1_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbk1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboot_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Google 드라이브/papers_related/graduation thesis/programs/ens_surv/boot_kfold.py\u001b[0m in \u001b[0;36mboot_stack\u001b[0;34m(self, train_df_list, test_df_list, model_specifics_1, model_specifics_2, ID_col, E_col, T_col, boot, B, K)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 out_b_k = level_1_stack(model_specifics_1,ID_col=ID_col, E_col=E_col, T_col = T_col,\n\u001b[1;32m    138\u001b[0m                                         \u001b[0mtrain_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf1_k_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2_k_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf3_k_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                                         validation_sets=[df1_k_validation, df2_k_validation, df3_k_validation])\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mb_TH_STACK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_TH_STACK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_b_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google 드라이브/papers_related/graduation thesis/programs/ens_surv/utils.py\u001b[0m in \u001b[0;36mlevel_1_stack\u001b[0;34m(model_specifics, ID_col, E_col, T_col, train_sets, validation_sets)\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;31m# print(model_instance.print_summary())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                 \u001b[0msurv_prob_est\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_year_survival_prob_cox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_years\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_years\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurv_prob_est\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'v_years' is not defined"
     ]
    }
   ],
   "source": [
    "bk1_stack = bk1.boot_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca93432-6620-4403-8501-1d10f09ccbef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936c643-1b67-4d3e-b9c6-a6b4cea81769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304b4c1-a982-4390-893e-495675651b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39714e3-33d5-4b01-a4b1-a0fc8d320168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec8b01-0351-40ee-aec6-da08ef12477c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4bb0c0-fffc-46ad-b388-fa69a26ec85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0828116-f5ef-41d3-a0f1-84e95e00ea2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9173181-5f89-409a-bedc-82c2053ae042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4133b2-c07a-4a5c-9e64-0628ea09a475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec67aa6-2369-457f-aeed-c894b1561f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c9102-0914-4d7a-96f9-b5c5aa489e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d2cdd-a75f-4399-aab3-1508954fc1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b1900-a760-4db4-9b68-366cc41fdbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a736db5-38fb-4f6d-99e2-46be68701c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f638948-98c0-4902-91e1-13572a8bf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_ipcw, concordance_index_censored\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# models \n",
    "from lifelines import CoxPHFitter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# others\n",
    "from numpy import inf\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "\n",
    "# ENS SURV module\n",
    "from ens_surv.utils import *\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# Caution ! : If boot = False, B should be 1\n",
    "\n",
    "class boot_kfold :\n",
    "    def __init(self, base_info, train_df_list, test_df_list,model_specifics_1, model_specifics_2) :         \n",
    "        # base_info : dict with ID_col, T_col, E_col, measure_T_col names, boot(bool), B, K\n",
    "        self.base_info = base_info\n",
    "        self.ID_col = base_info['ID_col']\n",
    "        self.T_col = base_info['T_col']\n",
    "        self.E_col = base_info['E_col']\n",
    "        self.measure_T_col = base_info['measure_T_col']\n",
    "        self.window = base_info['window']\n",
    "        self.S = base_info['S']\n",
    "        self.k_bin = base_info['k_bin']\n",
    "        \n",
    "        self.boot = base_info['boot']\n",
    "        self.B = base_info['B']\n",
    "        self.K = base_info['K']\n",
    "        \n",
    "        # sorting dataframes in right order\n",
    "        temp = []\n",
    "        for df in train_df_list :\n",
    "            temp.append(df.sort_values(['LM',ID_col]))\n",
    "        train_df_list = temp\n",
    "        \n",
    "        temp = []\n",
    "        for df in test_df_list :\n",
    "            temp.append(df.sort_values(['LM',ID_col]))\n",
    "        test_df_list = temp\n",
    "        \n",
    "        del(temp)\n",
    "\n",
    "        # list of dataframes :\n",
    "        ## in train, sequently, original data / lm1 transformed / lm2 transformed(trn form) / lm2 transformed(validation form)\n",
    "        ## in test, sequently, original data / lm1 transformed/ lm2 transformed(validation form)\n",
    "        self.train_df_list = train_df_list\n",
    "        self.test_df_list = test_df_list\n",
    "        \n",
    "        # model_specifics(dataframe)\n",
    "        ## model_specifics_1 : 1st stage models' 1) model name / model_instance / hyperparams grid / type\n",
    "        ## model_specifics_2 : 2nd stage models' 1) model name / model_instance / hyperparams grid / type\n",
    "        self.model_specifics_1 = model_specifics_1\n",
    "        self.model_specifics_2 = model_specifics_2\n",
    "    \n",
    "    # boot_stack outputs B stacked super set\n",
    "    def boot_stack(df_list = None, model_specifics_1 = None, ID_col = None, boot = None, B = None, K= None) : \n",
    "        # initiallizing\n",
    "        if df_list is None :\n",
    "            df_list = self.df_list\n",
    "        if model_specifics_1 is None :\n",
    "            model_specifics_1 = self.model_specifics_1\n",
    "        if ID_col is None :\n",
    "            ID_col = self.ID_col\n",
    "        if boot is None :\n",
    "            boot = self.boot\n",
    "        if B is None :\n",
    "            B = self.B\n",
    "        if K is None :\n",
    "            K = self.K\n",
    "            \n",
    "        #OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP#\n",
    "        # OUTER-LOOP\n",
    "        BOOTSTRAP_SUPERSETS = []\n",
    "        for b in range(B) :\n",
    "            print('######################################################################')\n",
    "            print(b+1,'/', B,' Resampled')\n",
    "            # boot_weight_at_b -> calculates number of inclusion in the b_th bootstrap sample of each exmaples.\n",
    "            boot_weight_at_b = boot_weight(df = df_list[0], ID_col = ID_col, boot=boot)\n",
    "            \n",
    "            # examples that are included in the bag\n",
    "            df_list_new = []\n",
    "            for df_temp in df_list :\n",
    "                df_temp = pd.merge(left= df_temp, right = boot_weight_at_b, how='left', on= ID_col)\n",
    "                df_temp = df_temp[df_temp['weight_boot'] != 0] # delete rows with ids exclouded\n",
    "                df_list_new.append(df_temp)\n",
    "            \n",
    "            # examples that are excluded from the bag\n",
    "            df_list_oob = []\n",
    "            for df_temp in df_list :\n",
    "                df_temp = pd.merge(left= df_temp, right = boot_weight_at_b, how='left', on= ID_col)\n",
    "                df_temp = df_temp[df_temp['weight_boot'] == 0] # delete rows with ids exclouded\n",
    "                df_temp = df_temp.drop(['weight_boot'],axis=1)\n",
    "                df_list_oob.append(df_temp)\n",
    "            \n",
    "            # kfold part - Different IDs are divided into K folds\n",
    "            kf = kfold(k=K, ID_col=ID_col, df1 = df_list_new[0], df2 = df_list_new[1], df3_train = df_list_new[2], df3_validation = df_list_new[3])\n",
    "\n",
    "            ############################################################################################################\n",
    "            # INNER-LOOP\n",
    "            ## b_TH_STACK : 1st column contains true survival status / 2 to end columns contain survival estimates(of training set) from different models. \n",
    "            b_TH_STACK = np.array([]) \n",
    "            for k in range(K) :\n",
    "                print(k+1,'/', K,' fold')\n",
    "                df1_k_train, df1_k_validation, df2_k_train, df2_k_validation, df3_k_train, df3_k_validation = next(kf)\n",
    "\n",
    "                # Training 1st stage models\n",
    "                ## 1) Training 1st stage models with kth training set\n",
    "                ## 2) Predict kth validation set with trained 1st stage models\n",
    "                ## Stacking results from 2), forming inputs for 2nd stage models\n",
    "\n",
    "                out_b_k = level_1_stack(model_specifics, \n",
    "                                        train_sets=[df1_k_train, df2_k_train, df3_k_train], \n",
    "                                        validation_sets=[df1_k_validation, df2_k_validation, df3_k_validation])\n",
    "\n",
    "                b_TH_STACK = b_TH_STACK.reshape(-1, out_b_k.shape[1])\n",
    "                b_TH_STACK = np.vstack((b_TH_STACK, out_b_k))\n",
    "                \n",
    "            ## BOOTSTRAP_SUPERSETS : All B (b_TH_STACK) super sets obtained from B bootstrap samples.\n",
    "            BOOTSTRAP_SUPERSETS.append(b_TH_STACK)\n",
    "            ############################################################################################################    \n",
    "            \n",
    "            \n",
    "            # Training 2nd stage models (for each B iteration of boostrapping)\n",
    "\n",
    "                ## Given BOOTSTRAP_SUPERSETS,\n",
    "                \n",
    "                ## 1) Train 2nd stage models B time following each algorithm using BOOTSTRAP_SUPERSETS \n",
    "                ## 2) oob score calculation( If boot=False, step 2) is ignored )\n",
    "                ### 2-1) Refit 1st stage models with whole training set\n",
    "                ### 2-2) Predict oob samples with Refited 1st stage models and obtain something like BOOTSTRAP_SUPERSETS.\n",
    "                ### 2-3) from output of 2-2), calculate bth 2nd stage prediction for b = 1, ... ,B\n",
    "                ### 2-4) averaging B outputs and obtain final oob prediction\n",
    "                \n",
    "                ## 3) test set calculation\n",
    "                ### 3-1) Predict test samples with 1st stage models and obtain  something like BOOTSTRAP_SUPERSETS.\n",
    "                ### 3-2) from output of 3-1), calculate bth 2nd stage prediction for b = 1, ... ,B\n",
    "                ### 3-3) averaging B outputs and obtain final prediction\n",
    "\n",
    "        #OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP##OUTER-LOOP#\n",
    "\n",
    "        # df1_k_train, df1_k_validation, df2_k_train, df2_k_validation, df3_k_train, df3_k_validation\n",
    "        return BOOTSTRAP_STACKS\n",
    "'''\n",
    "\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc439ebf-3a95-4e8c-9893-3caea332f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTIONS: \n",
    "## IF boot is false, then B should be 1 \n",
    "\n",
    "def boot_kfold(df_list, model_specifics ,ID_col, boot, B, k) : \n",
    "    # OUTER-LOOP\n",
    "    BOOTSTRAP_STACKS = []\n",
    "    for b in range(B) :\n",
    "        # bootstrapping -> calculate number of inclusion in the b_th bootstrap sample of each exmaples.\n",
    "        boot_weight_at_b = boot_weight(df = df_list[0], ID_col = ID_col, boot=boot)\n",
    "            \n",
    "        df_list_new = []\n",
    "        for df_temp in df_list :\n",
    "            df_temp = pd.merge(left= df_temp, right = boot_weight_at_b, how='left', on= ID_col)\n",
    "            df_temp = df_temp[df_temp['weight_boot'] != 0] # delete rows with ids exclouded\n",
    "            df_list_new.append(df_temp)\n",
    "            \n",
    "        # kfold part - Different IDs are divided into K folds\n",
    "        kf = kfold(k=K,ID_col=ID_col, df1 = df_list_new[0], df2 = df_list_new[1], df3_train = df_list_new[2], df3_validation = df_list_new[3])\n",
    "        \n",
    "        ############################################################################################################\n",
    "        # INNER-LOOP\n",
    "        b_TH_STACK = np.array([])\n",
    "        for k in range(K) :\n",
    "            df1_k_train, df1_k_validation, df2_k_train, df2_k_validation, df3_k_train, df3_k_validation = next(kf)\n",
    "            print(set(df3_k_validation[ID_col]))\n",
    "                \n",
    "            # Training 1st stage models\n",
    "            ## 1) Training 1st stage models with kth training set\n",
    "            ## 2) Predict kth validation set with trained 1st stage models\n",
    "            ## Stacking results from 2), forming inputs for 2nd stage models\n",
    "                \n",
    "            out_b_k = level_1_stack(model_specifics, \n",
    "                                    train_sets=[df1_k_train, df2_k_train, df3_k_train], \n",
    "                                    validation_sets=[df1_k_validation, df2_k_validation, df3_k_validation])\n",
    "            \n",
    "            b_TH_STACK = b_TH_STACK.reshape(-1, out_b_k.shape[1])\n",
    "            b_TH_STACK = np.vstack((b_TH_STACK, out_b_k))\n",
    "        ############################################################################################################    \n",
    "            # Training 2nd stage models\n",
    "            \n",
    "            # For each b iteration of boostrapping...\n",
    "            \n",
    "            ## Given stacked (prediction) results from 2), and stacked true values of kth validations set\n",
    "            ## 1) Train 2nd stage models following each algorithm\n",
    "                \n",
    "            ## 2) Predict bth oob samples with trained 2nd stage models. And see if there's sign of misfit.\n",
    "        BOOTSTRAP_STACKS.append(b_TH_STACK)\n",
    "            \n",
    "        # When Boot is not True\n",
    "        ## TO BE...\n",
    "    \n",
    "    # df1_k_train, df1_k_validation, df2_k_train, df2_k_validation, df3_k_train, df3_k_validation\n",
    "    return BOOTSTRAP_STACKS\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea6616-2a8d-4582-95fb-87caf0cba512",
   "metadata": {},
   "source": [
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87bbc1f-ade7-4bfb-af33-0fee1c7fdd33",
   "metadata": {},
   "source": [
    "# Plan of action \n",
    "\n",
    "## Models with K-folds and no bagging : No bootstrapping / K-fold(super set)\n",
    "- M1 : non-negative weighted linear regression \n",
    "- M2 : logistic regression with binary cross entropy loss\n",
    "- M3 : Ensemble selection(with replacement) a.k.a hill climbing\n",
    "- M4 : Another 2nd level models such as Lasso, RF, GB... \n",
    "\n",
    "## Models with k-folds and bagging : return averaged survival estimates from B bagged 2nd level models\n",
    "\n",
    "- M1' : M1 + bagging\n",
    "- M2' : M2 + bagging \n",
    "- M3' : M3 + bagging\n",
    "- M4' : M4 + bagging\n",
    "\n",
    "## Models with k-folds and bagging + different methods\n",
    "\n",
    "- M5(PROPOSE) : Ensemble Selection(with replacement) + stepwise Bagging\n",
    "    - M3' + Stepwise selection\n",
    "    - For every b, b = 1, 2, 3, ... , B, super set is obtained thru k-folds\n",
    "    - And Ensemble \"STEPWISE\" Selection on super set\n",
    "    - Stopping when score in oob samples are converged.\n",
    "    - 장점 : overfitting 여부 예측 가능. When to stop?에 대한 해결책 제공\n",
    "    \n",
    "## Models with Gate controll\n",
    "- M6(PROPOSE) : Gate control fusion \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf8c47c-6657-4c8c-839c-c556a6e6927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitID(data = data, ID = ID_col, p = p_train) :\n",
    "    # Unique ID names\n",
    "    unique_ids = np.unique(data[ID_col])\n",
    "\n",
    "    # Number of samples within each train and test set\n",
    "    n_train = round(len(unique_ids)*0.7)\n",
    "    n_test = len(unique_ids) - n_train\n",
    "    \n",
    "    # IDs within train set and test set\n",
    "    train_ids = list(sample(set(unique_ids), n_train))\n",
    "    test_ids = list(set(unique_ids).difference(set(train_ids)))\n",
    "\n",
    "    # Row-wise masking for train and test set\n",
    "    mask_train = data[ID_col].isin(train_ids)\n",
    "    mask_test = data[ID_col].isin(test_ids)\n",
    "\n",
    "    # final train and test sets\n",
    "    data_train = data[mask_train].reset_index(drop=True)\n",
    "    data_test = data[mask_test].reset_index(drop=True)\n",
    "    \n",
    "    return data_train, data_test\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bb57c-e38f-4abb-9672-3dc30738d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = splitID(data = data, ID = ID_col, p = p_train)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print('Intersection : ', set(np.unique(train[ID_col])).intersection(set(np.unique(test[ID_col]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23cfe53-7f42-4202-acbd-a8634a797300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, test = splitID(data = data, ID = ID_col, p = p_train)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print('Intersection : ', set(np.unique(train[ID_col])).intersection(set(np.unique(test[ID_col]))))\n",
    "\n",
    "train_lm1 = LM_transformer(df=train)\n",
    "test_lm1 = LM_transformer(df=test)\n",
    "\n",
    "# \n",
    "train_lm2_train_ver = LM_transformer2(df=train_lm1,train=True)\n",
    "train_lm2_validation_ver = LM_transformer2(df=train_lm1,train=False)\n",
    "\n",
    "test_lm2 = LM_transformer2(df=test_lm1,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ac08b-75fd-44de-b611-17116b77fe84",
   "metadata": {},
   "source": [
    "# Models with K-folds and no bagging : No bootstrapping / K-fold(super set)\n",
    "- M1 : non-negative weighted linear regression \n",
    "- M2 : logistic regression with binary cross entropy loss\n",
    "- M3 : Ensemble selection(with replacement) a.k.a hill climbing\n",
    "- M4 : Another 2nd level models such as Lasso, RF, GB... \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804052d-f0cb-4ddf-bebb-8ced4ff5feb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141ae3a-9141-4682-b0bb-02f15d2bea5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd11b2-0a84-4a3b-b85a-02ab37bef9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc0bcc7-0545-4d98-99c5-97bde02b33a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e05eef-ac98-45ab-a738-48527eaf1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_STACKS_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274d6f1-717b-4a0d-992e-713936bb30bc",
   "metadata": {},
   "source": [
    "## M1 : non-negative weighted linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6fbedd-5389-4960-8f07-6c8f994617ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "641b158c-92a1-4da9-bcb4-cddce4da24e0",
   "metadata": {},
   "source": [
    "## M2 : logistic regression with binary cross entropy loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725aa0c-469f-44ef-aff7-c6635ceb4498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9919ef48-3757-4c9b-b555-5ba0010a19aa",
   "metadata": {},
   "source": [
    "## M3 : Ensemble selection(with replacement) a.k.a hill climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea6dc5-3b95-4cc7-9451-634fd1647fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "271594b8-33b8-48b9-88d6-4c305168029a",
   "metadata": {},
   "source": [
    "## M4 : Another 2nd level models such as Lasso, RF, GB..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9db52-b655-4e9f-9b9d-ce0305d28695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2d97e-4e5e-4f15-8278-26e16eca2265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f6517-c0ed-4fa3-96e5-4c25942be3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86650e74-3edb-48c9-ae64-8812cd4068cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
